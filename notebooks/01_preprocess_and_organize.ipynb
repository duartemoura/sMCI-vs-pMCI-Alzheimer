{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Organize Raw ADNI Data from CSV\n",
    "\n",
    "This notebook organizes the raw ADNI dataset using the provided CSV file that contains metadata about each neuroimaging file. The CSV contains information about Image Data ID, Subject, Group, and other metadata.\n",
    "\n",
    "**CSV Structure:**\n",
    "- `Image Data ID`: Unique identifier for each image (e.g., 'I325923')\n",
    "- `Subject`: Subject identifier (e.g., '116_S_4855')\n",
    "- `Group`: Research group classification (AD, CN, or MCI)\n",
    "- Other columns: Sex, Age, Visit, Modality, etc.\n",
    "\n",
    "The script will organize NIfTI files into separate folders based on the research group classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file containing the metadata\n",
    "csv_file = Path(\"PATH_TO_DATA\")\n",
    "\n",
    "# Base path where your NIfTI files are located\n",
    "# Update this to point to your actual ADNI data directory\n",
    "data_base_path = Path(\"PATH_TO_DATA\")\n",
    "\n",
    "# Output path for organized data\n",
    "output_path = Path(\"PATH_TO_DATA\")\n",
    "ad_path = output_path / \"ad\"\n",
    "cn_path = output_path / \"cn\"\n",
    "mci_path = output_path / \"mci\"\n",
    "\n",
    "# Create output directories\n",
    "ad_path.mkdir(parents=True, exist_ok=True)\n",
    "cn_path.mkdir(parents=True, exist_ok=True)\n",
    "mci_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"CSV file: {csv_file}\")\n",
    "print(f\"Data base path: {data_base_path}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Inspect CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file - try different parsing approaches\n",
    "try:\n",
    "    # First, try with comma delimiter (standard)\n",
    "    df = pd.read_csv(csv_file, delimiter=',')\n",
    "    print(f\"Loaded CSV with {len(df)} rows using comma delimiter\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    print(df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nGroup distribution:\")\n",
    "    print(df['Group'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Error with comma delimiter: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Try with semicolon delimiter\n",
    "        df = pd.read_csv(csv_file, delimiter=';')\n",
    "        print(f\"Loaded CSV with {len(df)} rows using semicolon delimiter\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(df.columns.tolist())\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Check if we have the malformed single column issue\n",
    "        if len(df.columns) == 1 and ',' in df.columns[0]:\n",
    "            print(\"\\nDetected malformed CSV - column names are in one cell.\")\n",
    "            print(\"Attempting to fix...\")\n",
    "            \n",
    "            # Extract the column names from the first column header\n",
    "            header_col = df.columns[0]\n",
    "            column_names = [col.strip().strip('\"') for col in header_col.split(',')]\n",
    "            print(f\"Extracted column names: {column_names}\")\n",
    "            \n",
    "            # Read the CSV again with proper column names\n",
    "            df = pd.read_csv(csv_file, delimiter=';', names=column_names, skiprows=1)\n",
    "            print(f\"Fixed CSV with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            print(\"\\nColumn names:\")\n",
    "            print(df.columns.tolist())\n",
    "            print(\"\\nFirst few rows:\")\n",
    "            print(df.head())\n",
    "        \n",
    "        print(\"\\nGroup distribution:\")\n",
    "        print(df['Group'].value_counts())\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Error with semicolon delimiter: {e2}\")\n",
    "        print(\"Please check the file path and format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing essential information\n",
    "initial_count = len(df)\n",
    "df_clean = df.dropna(subset=['Image Data ID', 'Subject', 'Group'])\n",
    "dropped_count = initial_count - len(df_clean)\n",
    "\n",
    "print(f\"Dropped {dropped_count} rows with missing essential data\")\n",
    "print(f\"Remaining: {len(df_clean)} rows\")\n",
    "\n",
    "# Make baseline-only filtering optional with a flag\n",
    "BASELINE_ONLY = False  # <-- Set this to True to only use baseline scans for MCI/EMCI/LMCI\n",
    "\n",
    "print(\"\\n=== MCI PROCESSING (includes MCI, EMCI, LMCI) ===\")\n",
    "mci_types = ['MCI', 'EMCI', 'LMCI']\n",
    "mci_before = len(df_clean[df_clean['Group'].isin(mci_types)])\n",
    "print(f\"MCI/EMCI/LMCI entries before processing: {mci_before}\")\n",
    "\n",
    "# Show breakdown by type\n",
    "mci_breakdown = df_clean[df_clean['Group'].isin(mci_types)]['Group'].value_counts()\n",
    "print(f\"Breakdown: {dict(mci_breakdown)}\")\n",
    "\n",
    "if BASELINE_ONLY:\n",
    "    # For MCI patients (all types), filter to only baseline/initial scans\n",
    "    mci_mask = df_clean['Group'].isin(mci_types)\n",
    "    mci_patients = df_clean[mci_mask].copy()\n",
    "\n",
    "    # Filter MCI patients to only include visits with 'bl' or 'init'\n",
    "    mci_baseline_mask = mci_patients['Visit'].str.contains('bl|init', case=False, na=False)\n",
    "    mci_baseline = mci_patients[mci_baseline_mask].copy()\n",
    "\n",
    "    # For each MCI subject, keep only the first baseline/initial scan\n",
    "    mci_filtered = mci_baseline.sort_values(['Subject', 'Visit']).groupby('Subject').first().reset_index()\n",
    "\n",
    "    # Normalize all MCI types to 'MCI' for consistency\n",
    "    mci_filtered['Group'] = 'MCI'\n",
    "\n",
    "    print(f\"MCI entries after baseline filtering: {len(mci_baseline)}\")\n",
    "    print(f\"MCI entries after keeping only first baseline per subject: {len(mci_filtered)}\")\n",
    "\n",
    "    # Combine non-MCI data with filtered MCI data\n",
    "    non_mci_data = df_clean[~df_clean['Group'].isin(mci_types)]\n",
    "    df_clean = pd.concat([non_mci_data, mci_filtered], ignore_index=True)\n",
    "\n",
    "    mci_after = len(df_clean[df_clean['Group'] == 'MCI'])\n",
    "    print(f\"Final MCI entries (normalized from MCI/EMCI/LMCI): {mci_after}\")\n",
    "    print(f\"Total MCI entries removed: {mci_before - mci_after}\")\n",
    "else:\n",
    "    # Normalize all MCI types to 'MCI' for consistency, but keep all scans/rows\n",
    "    df_clean['Group'] = df_clean['Group'].replace({'EMCI': 'MCI', 'LMCI': 'MCI'})\n",
    "\n",
    "    mci_after = len(df_clean[df_clean['Group'] == 'MCI'])\n",
    "    print(f\"Final MCI entries (normalized from MCI/EMCI/LMCI): {mci_after}\")\n",
    "    print(f\"Total MCI entries removed: {mci_before - mci_after}\")  # This will likely be 0\n",
    "\n",
    "# Show some statistics\n",
    "print(\"\\n=== DATA STATISTICS ===\")\n",
    "print(f\"Unique subjects: {df_clean['Subject'].nunique()}\")\n",
    "print(f\"Unique images: {df_clean['Image Data ID'].nunique()}\")\n",
    "print(\"\\nGroup distribution:\")\n",
    "print(df_clean['Group'].value_counts())\n",
    "\n",
    "print(\"\\nSample subjects by group:\")\n",
    "for group in ['AD', 'CN', 'MCI']:\n",
    "    subjects = df_clean[df_clean['Group'] == group]['Subject'].unique()[:3]\n",
    "    print(f\"  {group}: {subjects}\")\n",
    "\n",
    "if BASELINE_ONLY:\n",
    "    print(\"\\nMCI visit types (should only be baseline/initial):\")\n",
    "else:\n",
    "    print(\"\\nMCI visit types (all visit types for MCI retained):\")\n",
    "\n",
    "if len(df_clean[df_clean['Group'] == 'MCI']) > 0:\n",
    "    mci_visits = df_clean[df_clean['Group'] == 'MCI']['Visit'].value_counts()\n",
    "    print(mci_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Finding Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nifti_file(subject_id, image_id, base_path, acq_date=None, description=None):\n",
    "    \"\"\"Find the NIfTI file for a given subject and image ID.\n",
    "    \n",
    "    Since Image Data ID doesn't appear in filenames, we use:\n",
    "    1. Subject ID to find the directory\n",
    "    2. Acquisition date to match the scan date folder\n",
    "    3. Description to match scan type (FDG, PIB, etc.)\n",
    "    \n",
    "    File structure: base_path/SUBJECT_ID/SCAN_TYPE/DATE/FILENAME.nii.gz\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # First, look for the subject directory\n",
    "    subject_path = base_path / subject_id\n",
    "    if not subject_path.exists():\n",
    "        print(f\"  Warning: Subject directory not found: {subject_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Search for any .nii or .nii.gz files in subject's subdirectories\n",
    "    nifti_files = []\n",
    "    for pattern in [\"**/*.nii\", \"**/*.nii.gz\"]:\n",
    "        nifti_files.extend(list(subject_path.glob(pattern)))\n",
    "    \n",
    "    if not nifti_files:\n",
    "        print(f\"  Warning: No NIfTI files found for subject {subject_id}\")\n",
    "        return None\n",
    "    \n",
    "    # If only one file, return it\n",
    "    if len(nifti_files) == 1:\n",
    "        return nifti_files[0]\n",
    "    \n",
    "    # Multiple files found - try to match using acquisition date and description\n",
    "    print(f\"  Info: Found {len(nifti_files)} files for {subject_id}\")\n",
    "    \n",
    "    best_matches = []\n",
    "    \n",
    "    # Try to match by acquisition date if provided\n",
    "    if acq_date:\n",
    "        try:\n",
    "            # Parse the acquisition date (format: M/D/YYYY)\n",
    "            target_date = datetime.strptime(acq_date, \"%m/%d/%Y\")\n",
    "            target_date_str = target_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            for file_path in nifti_files:\n",
    "                # Check if date folder matches\n",
    "                date_folder = file_path.parent.name  # e.g., \"2011-06-09_08_23_48.0\"\n",
    "                if target_date_str in date_folder:\n",
    "                    best_matches.append(file_path)\n",
    "                    \n",
    "        except ValueError:\n",
    "            print(f\"  Warning: Could not parse acquisition date: {acq_date}\")\n",
    "    \n",
    "    # If we found date matches, use those\n",
    "    if best_matches:\n",
    "        if len(best_matches) == 1:\n",
    "            print(f\"  ✓ Matched by date: {best_matches[0].name}\")\n",
    "            return best_matches[0]\n",
    "        else:\n",
    "            print(f\"  Info: Multiple date matches, trying description filter...\")\n",
    "            nifti_files = best_matches\n",
    "    \n",
    "    # Try to match by description/modality (FDG, PIB, etc.)\n",
    "    if description:\n",
    "        desc_upper = description.upper()\n",
    "        modality_matches = []\n",
    "        \n",
    "        for file_path in nifti_files:\n",
    "            file_upper = str(file_path).upper()\n",
    "            # Check for common scan types\n",
    "            if 'FDG' in desc_upper and 'FDG' in file_upper:\n",
    "                modality_matches.append(file_path)\n",
    "            elif 'PIB' in desc_upper and 'PIB' in file_upper:\n",
    "                modality_matches.append(file_path)\n",
    "            elif 'AMYLOID' in desc_upper and ('PIB' in file_upper or 'AMYLOID' in file_upper):\n",
    "                modality_matches.append(file_path)\n",
    "        \n",
    "        if modality_matches:\n",
    "            print(f\"  ✓ Matched by modality: {modality_matches[0].name}\")\n",
    "            return modality_matches[0]\n",
    "    \n",
    "    # If no specific matches, return the first file but warn\n",
    "    print(f\"  Warning: Multiple files, no clear match. Using first file:\")\n",
    "    for i, f in enumerate(nifti_files[:3]):  # Show first 3\n",
    "        print(f\"    {i+1}. {f.name}\")\n",
    "    if len(nifti_files) > 3:\n",
    "        print(f\"    ... and {len(nifti_files)-3} more\")\n",
    "    \n",
    "    return nifti_files[0]\n",
    "\n",
    "def get_destination_path(group, subject_id, image_id, source_file):\n",
    "    \"\"\"Get the destination path for a file based on group.\"\"\"\n",
    "    if group == 'AD':\n",
    "        dest_dir = ad_path\n",
    "    elif group == 'CN':\n",
    "        dest_dir = cn_path\n",
    "    elif group in ['MCI', 'EMCI', 'LMCI']:\n",
    "        # All MCI types go to the same MCI folder\n",
    "        dest_dir = mci_path\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # Preserve the original file extension (.nii or .nii.gz)\n",
    "    original_name = source_file.name\n",
    "    if original_name.endswith('.nii.gz'):\n",
    "        extension = '.nii.gz'\n",
    "    elif original_name.endswith('.nii'):\n",
    "        # Check if it's actually compressed despite .nii extension\n",
    "        try:\n",
    "            with open(source_file, 'rb') as f:\n",
    "                header = f.read(2)\n",
    "            if header == b'\\x1f\\x8b':  # gzip magic number\n",
    "                extension = '.nii.gz'\n",
    "                print(f\"  Note: Correcting extension for compressed file: {original_name}\")\n",
    "            else:\n",
    "                extension = '.nii'\n",
    "        except:\n",
    "            extension = '.nii'  # fallback\n",
    "    else:\n",
    "        extension = '.nii'  # fallback\n",
    "    \n",
    "    # Create standardized filename: Subject_ImageID.extension\n",
    "    dest_filename = f\"{subject_id}_{image_id}{extension}\"\n",
    "    return dest_dir / dest_filename\n",
    "\n",
    "# Test the function with a few examples\n",
    "print(\"Testing file search with first few entries...\")\n",
    "print(f\"Base path: {data_base_path}\")\n",
    "print(f\"Base path exists: {data_base_path.exists()}\")\n",
    "print()\n",
    "\n",
    "test_rows = df_clean.head(5)\n",
    "for _, row in test_rows.iterrows():\n",
    "    subject_id = row['Subject']\n",
    "    image_id = row['Image Data ID']\n",
    "    group = row['Group']\n",
    "    acq_date = row.get('Acq Date', None)\n",
    "    description = row.get('Description', None)\n",
    "    \n",
    "    print(f\"Testing: {subject_id} / {image_id} / {group}\")\n",
    "    print(f\"  Acq Date: {acq_date}, Description: {description}\")\n",
    "    \n",
    "    found_file = find_nifti_file(subject_id, image_id, data_base_path, acq_date, description)\n",
    "    if found_file:\n",
    "        print(f\"  ✓ Found: {found_file}\")\n",
    "        dest_path = get_destination_path(group, subject_id, image_id, found_file)\n",
    "        print(f\"  → Would copy to: {dest_path}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Not found\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row in the CSV\n",
    "# Note: MCI patients are filtered to only include baseline/initial scans\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "group_counts = {'AD': 0, 'CN': 0, 'MCI': 0}\n",
    "errors = []\n",
    "skipped_duplicates = 0\n",
    "\n",
    "print(\"Starting file organization...\")\n",
    "print(f\"Processing {len(df_clean)} entries...\")\n",
    "print(\"Note: MCI patients filtered to only baseline/initial scans\\n\")\n",
    "\n",
    "for idx, row in tqdm(df_clean.iterrows(), total=len(df_clean), desc=\"Organizing files\"):\n",
    "    try:\n",
    "        # Extract information from row\n",
    "        subject_id = row['Subject']\n",
    "        image_id = row['Image Data ID']\n",
    "        group = row['Group']\n",
    "        acq_date = row.get('Acq Date', None)\n",
    "        description = row.get('Description', None)\n",
    "        \n",
    "        # Skip if group is not one of the expected values (MCI types are normalized to 'MCI' earlier)\n",
    "        if group not in ['AD', 'CN', 'MCI', 'EMCI', 'LMCI']:\n",
    "            errors.append(f\"Row {idx}: Invalid group '{group}' for subject {subject_id}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Find the source NIfTI file\n",
    "        source_file = find_nifti_file(subject_id, image_id, data_base_path, acq_date, description)\n",
    "        if source_file is None:\n",
    "            errors.append(f\"Row {idx}: File not found for {subject_id}/{image_id}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Get destination path\n",
    "        dest_path = get_destination_path(group, subject_id, image_id, source_file)\n",
    "        if dest_path is None:\n",
    "            errors.append(f\"Row {idx}: Invalid group '{group}'\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Copy file if it doesn't exist\n",
    "        if dest_path.exists():\n",
    "            skipped_duplicates += 1\n",
    "        else:\n",
    "            shutil.copy2(source_file, dest_path)\n",
    "        \n",
    "        group_counts[group] += 1\n",
    "        success_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        errors.append(f\"Row {idx}: Unexpected error - {str(e)}\")\n",
    "        error_count += 1\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ORGANIZATION SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Successfully processed: {success_count} files\")\n",
    "print(f\"Errors encountered: {error_count} files\")\n",
    "print(f\"Skipped duplicates: {skipped_duplicates} files\")\n",
    "print(f\"\\nFiles organized by group:\")\n",
    "for group, count in group_counts.items():\n",
    "    print(f\"  {group}: {count} files\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nFirst 10 errors:\")\n",
    "    for error in errors[:10]:\n",
    "        print(f\"  {error}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"  ... and {len(errors) - 10} more errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the organization by counting actual files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "actual_counts = {}\n",
    "all_organized_files = []\n",
    "\n",
    "for group_name, group_path in [('AD', ad_path), ('CN', cn_path), ('MCI', mci_path)]:\n",
    "    files = list(group_path.glob('*.nii*'))\n",
    "    actual_counts[group_name] = len(files)\n",
    "    all_organized_files.extend(files)\n",
    "    print(f\"\\n{group_name} folder:\")\n",
    "    print(f\"  Files found: {len(files)}\")\n",
    "    if files:\n",
    "        print(f\"  Example files: {[f.name for f in files[:3]]}\")\n",
    "        if len(files) > 3:\n",
    "            print(f\"  ... and {len(files) - 3} more\")\n",
    "\n",
    "total_organized = sum(actual_counts.values())\n",
    "print(f\"\\nTotal files organized: {total_organized}\")\n",
    "print(f\"Total CSV entries: {len(df_clean)}\")\n",
    "\n",
    "# Better success rate calculation\n",
    "csv_entries_processed = success_count  # From the processing loop\n",
    "print(f\"CSV entries successfully processed: {csv_entries_processed}\")\n",
    "print(f\"Processing success rate: {csv_entries_processed/len(df_clean)*100:.1f}%\")\n",
    "\n",
    "# Analyze subjects with multiple scans - both in CSV and in organized files\n",
    "print(f\"\\n\" + \"=\"*30)\n",
    "print(\"MULTIPLE SCANS ANALYSIS\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# CSV analysis\n",
    "subject_counts_csv = df_clean['Subject'].value_counts()\n",
    "multiple_scans_csv = subject_counts_csv[subject_counts_csv > 1]\n",
    "\n",
    "# Organized files analysis - extract subject IDs from organized filenames\n",
    "organized_subjects = []\n",
    "for file_path in all_organized_files:\n",
    "    # Extract subject ID from filename (assuming format: SUBJECT_ID_*.nii)\n",
    "    filename = file_path.name\n",
    "    if '_' in filename:\n",
    "        subject_id = filename.split('_')[0] + '_' + filename.split('_')[1] + '_' + filename.split('_')[2]\n",
    "        organized_subjects.append(subject_id)\n",
    "\n",
    "from collections import Counter\n",
    "organized_subject_counts = Counter(organized_subjects)\n",
    "multiple_scans_organized = {subj: count for subj, count in organized_subject_counts.items() if count > 1}\n",
    "\n",
    "print(f\"Subjects with multiple scans in CSV: {len(multiple_scans_csv)}\")\n",
    "print(f\"Subjects with multiple scans organized: {len(multiple_scans_organized)}\")\n",
    "\n",
    "if multiple_scans_organized:\n",
    "    print(f\"\\nTop subjects with most organized scans:\")\n",
    "    sorted_subjects = sorted(multiple_scans_organized.items(), key=lambda x: x[1], reverse=True)\n",
    "    for subj, count in sorted_subjects[:5]:\n",
    "        csv_count = subject_counts_csv.get(subj, 0)\n",
    "        print(f\"  {subj}: {count} organized files (from {csv_count} CSV entries)\")\n",
    "\n",
    "# Check for any mismatches\n",
    "print(f\"\\n\" + \"=\"*25)\n",
    "print(\"CONSISTENCY CHECK\")\n",
    "print(\"=\"*25)\n",
    "total_unique_subjects_csv = len(df_clean['Subject'].unique())\n",
    "total_unique_subjects_organized = len(set(organized_subjects))\n",
    "print(f\"Unique subjects in CSV: {total_unique_subjects_csv}\")\n",
    "print(f\"Unique subjects organized: {total_unique_subjects_organized}\")\n",
    "\n",
    "if total_unique_subjects_organized < total_unique_subjects_csv:\n",
    "    missing_subjects = set(df_clean['Subject'].unique()) - set(organized_subjects)\n",
    "    print(f\"⚠️  Missing subjects: {len(missing_subjects)}\")\n",
    "    if len(missing_subjects) <= 10:\n",
    "        print(f\"  Missing: {list(missing_subjects)}\")\n",
    "    else:\n",
    "        print(f\"  First 10 missing: {list(missing_subjects)[:10]}\")\n",
    "        \n",
    "print(\"\\nNote: Multiple scans per subject are EXPECTED and beneficial for longitudinal analysis.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA ORGANIZATION COMPLETED!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Verify the file organization looks correct\")\n",
    "print(\"2. Run the next notebook: 02_preprocess_3d.ipynb\")\n",
    "print(\"3. For MCI data with multiple scans, consider using 04_mci_conversion_split_improved.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
