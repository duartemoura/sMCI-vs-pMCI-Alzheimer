{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 3D Preprocessing of AD and CN Data\n",
    "\n",
    "This notebook takes the organized AD and CN neuroimages and performs the necessary 3D preprocessing steps. The pipeline for each image is as follows:\n",
    "\n",
    "1.  **Patient Limiting**: Limits the dataset to a maximum of 300 AD subjects and 300 CN subjects (if available) to balance computational efficiency with dataset size.\n",
    "2.  **Resizing**: Each 3D image is resized to a standard size of (91, 109, 91) using trilinear interpolation for computational efficiency.\n",
    "3.  **Normalization**: The intensity values are normalized to a [0, 1] range.\n",
    "4.  **Data Splitting**: The processed images are split into training, validation, and test sets (80/10/10 split) at the subject level to prevent data leakage.\n",
    "5.  **Saving**: The final processed data (images and labels) for each set is saved as a pickled dictionary containing PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "import cc3d\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base paths for input and output\n",
    "base_path = Path(\"PATH_TO_DATA\")\n",
    "output_path = Path(\"PATH_TO_DATA\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input data paths\n",
    "ad_path = base_path / \"ad_stripped\"\n",
    "cn_path = base_path / \"cn_stripped\"\n",
    "\n",
    "# Verify paths exist and show file counts\n",
    "print(f\"AD path: {ad_path}\")\n",
    "print(f\"AD path exists: {ad_path.exists()}\")\n",
    "if ad_path.exists():\n",
    "    ad_files_count = len(list(ad_path.glob('*.nii*')))\n",
    "    print(f\"AD .nii files found: {ad_files_count}\")\n",
    "\n",
    "print(f\"\\nCN path: {cn_path}\")\n",
    "print(f\"CN path exists: {cn_path.exists()}\")\n",
    "if cn_path.exists():\n",
    "    cn_files_count = len(list(cn_path.glob('*.nii*')))\n",
    "    print(f\"CN .nii files found: {cn_files_count}\")\n",
    "\n",
    "# Output data paths for train, validation, and test sets\n",
    "train_path = output_path / \"train\"\n",
    "val_path = output_path / \"val\"\n",
    "test_path = output_path / \"test\"\n",
    "\n",
    "# Create the output directories\n",
    "train_path.mkdir(exist_ok=True)\n",
    "val_path.mkdir(exist_ok=True)\n",
    "test_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "def resize_volume(volume, target_shape=(100, 100, 90)):\n",
    "    \"\"\"\n",
    "    Resize 3D volume to target shape using trilinear interpolation.\n",
    "    \n",
    "    Args:\n",
    "        volume: 3D numpy array\n",
    "        target_shape: tuple of target dimensions (height, width, depth)\n",
    "    \n",
    "    Returns:\n",
    "        Resized 3D numpy array\n",
    "    \"\"\"\n",
    "    current_shape = volume.shape\n",
    "    zoom_factors = [target_shape[i] / current_shape[i] for i in range(3)]\n",
    "    return zoom(volume, zoom_factors, order=1)  # order=1 for trilinear interpolation\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Load a NIfTI image, resize to 91x109x91, and normalize intensities to [0,1].\n",
    "    - Assumes the image is already standardized (ADNI PET).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the NIfTI image\n",
    "        input_img = nib.load(image_path)\n",
    "        img_data = input_img.get_fdata()\n",
    "\n",
    "        # Handle 4D images by taking the first volume\n",
    "        if img_data.ndim == 4:\n",
    "            print(f\"Note: 4D image detected, using first volume: {image_path.name}\")\n",
    "            img_data = img_data[..., 0]\n",
    "        elif img_data.ndim != 3:\n",
    "            print(f\"Error: Unsupported image dimensions {img_data.shape}: {image_path.name}\")\n",
    "            return None\n",
    "\n",
    "        # Resize to target dimensions (100, 100, 90) for computational efficiency\n",
    "        img_data = resize_volume(img_data, target_shape=(100, 100, 90))\n",
    "\n",
    "        # Intensity normalization to [0,1]\n",
    "        img_min, img_max = img_data.min(), img_data.max()\n",
    "        if img_max > img_min:  # avoid divide-by-zero\n",
    "            img_data = (img_data - img_min) / (img_max - img_min)\n",
    "        else:\n",
    "            img_data = np.zeros_like(img_data)\n",
    "\n",
    "        return img_data.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_and_save(image_paths, labels, output_file):\n",
    "    \"\"\"\n",
    "    Process a list of images and save them as a pickled dictionary of PyTorch tensors.\n",
    "    Adds a channel dimension for 3D CNNs.\n",
    "    \"\"\"\n",
    "    processed_images = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for i, image_path in enumerate(tqdm(image_paths, desc=f\"Processing {output_file.name}\")):\n",
    "        processed = preprocess_image(image_path)\n",
    "        if processed is not None:\n",
    "            processed_images.append(np.expand_dims(processed, axis=0))  # (1, D, H, W)\n",
    "            valid_labels.append(labels[i])\n",
    "\n",
    "    if processed_images:\n",
    "        data = {\n",
    "            \"images\": torch.tensor(np.array(processed_images)),\n",
    "            \"labels\": torch.tensor(valid_labels)\n",
    "        }\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "        print(f\"Saved {len(processed_images)} images to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all files and organize by subject to prevent data leakage\n",
    "print(\"Loading and organizing files by subject...\")\n",
    "\n",
    "# Load AD files\n",
    "ad_files = sorted([f for f in ad_path.glob('*.nii*')])  # Matches both .nii and .nii.gz\n",
    "\n",
    "print(f\"Found {len(ad_files)} AD files\")\n",
    "\n",
    "# Load CN files  \n",
    "cn_files = sorted([f for f in cn_path.glob('*.nii*')])  # Matches both .nii and .nii.gz\n",
    "print(f\"Found {len(cn_files)} CN files\")\n",
    "\n",
    "if len(ad_files) == 0 and len(cn_files) == 0:\n",
    "    print(\"❌ ERROR: No .nii files found! Make sure you've run the organization notebook first.\")\n",
    "    print(\"Expected files in:\")\n",
    "    print(f\"  - {ad_path}\")\n",
    "    print(f\"  - {cn_path}\")\n",
    "else:\n",
    "    print(f\"✓ Total files found: {len(ad_files) + len(cn_files)}\")\n",
    "\n",
    "# Extract subject IDs and group files by subject to prevent data leakage\n",
    "def extract_subject_id(filename):\n",
    "    \"\"\"Extract subject ID from filename (format: SUBJECT_ID_*.nii)\"\"\"\n",
    "    parts = filename.stem.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        return f\"{parts[0]}_{parts[1]}_{parts[2]}\"  # e.g., \"002_S_0295\"\n",
    "    return filename.stem\n",
    "\n",
    "# Group files by subject - handle subjects with multiple diagnoses\n",
    "subject_files = {}\n",
    "\n",
    "# Process AD files\n",
    "for file_path in ad_files:\n",
    "    subject_id = extract_subject_id(file_path)\n",
    "    if subject_id not in subject_files:\n",
    "        subject_files[subject_id] = {'files': [], 'labels': []}\n",
    "    subject_files[subject_id]['files'].append(file_path)\n",
    "    subject_files[subject_id]['labels'].append(1)  # AD = 1\n",
    "\n",
    "# Process CN files\n",
    "for file_path in cn_files:\n",
    "    subject_id = extract_subject_id(file_path)\n",
    "    if subject_id not in subject_files:\n",
    "        subject_files[subject_id] = {'files': [], 'labels': []}\n",
    "    subject_files[subject_id]['files'].append(file_path)\n",
    "    subject_files[subject_id]['labels'].append(0)  # CN = 0\n",
    "\n",
    "print(f\"\\nFound {len(subject_files)} unique subjects\")\n",
    "\n",
    "# Limit to 300 subjects per class\n",
    "MAX_AD_SUBJECTS = 400\n",
    "MAX_CN_SUBJECTS = 400\n",
    "\n",
    "# Separate subjects by diagnosis\n",
    "ad_subjects = {subj: data for subj, data in subject_files.items() if 1 in data['labels']}\n",
    "cn_subjects = {subj: data for subj, data in subject_files.items() if 0 in data['labels'] and 1 not in data['labels']}\n",
    "\n",
    "print(f\"\\nBefore limiting:\")\n",
    "print(f\"  AD subjects: {len(ad_subjects)}\")\n",
    "print(f\"  CN subjects: {len(cn_subjects)}\")\n",
    "\n",
    "# Randomly select subjects to keep the limit\n",
    "random.seed(42)\n",
    "\n",
    "if len(ad_subjects) > MAX_AD_SUBJECTS:\n",
    "    ad_subject_ids = list(ad_subjects.keys())\n",
    "    random.shuffle(ad_subject_ids)\n",
    "    ad_subject_ids = ad_subject_ids[:MAX_AD_SUBJECTS]\n",
    "    ad_subjects = {subj: ad_subjects[subj] for subj in ad_subject_ids}\n",
    "    print(f\"  → Limited AD subjects to {len(ad_subjects)}\")\n",
    "\n",
    "if len(cn_subjects) > MAX_CN_SUBJECTS:\n",
    "    cn_subject_ids = list(cn_subjects.keys())\n",
    "    random.shuffle(cn_subject_ids)\n",
    "    cn_subject_ids = cn_subject_ids[:MAX_CN_SUBJECTS]\n",
    "    cn_subjects = {subj: cn_subjects[subj] for subj in cn_subject_ids}\n",
    "    print(f\"  → Limited CN subjects to {len(cn_subjects)}\")\n",
    "\n",
    "# Combine the limited subject sets\n",
    "subject_files = {**ad_subjects, **cn_subjects}\n",
    "print(f\"\\nAfter limiting: {len(subject_files)} total subjects\")\n",
    "\n",
    "# Show subjects with multiple diagnoses (this is expected in longitudinal studies)\n",
    "multi_diagnosis_subjects = {subj: data for subj, data in subject_files.items() \n",
    "                           if len(set(data['labels'])) > 1}\n",
    "print(f\"Subjects with multiple diagnoses: {len(multi_diagnosis_subjects)}\")\n",
    "if multi_diagnosis_subjects:\n",
    "    print(\"This is expected in longitudinal studies (e.g., CN → AD progression)\")\n",
    "    print(\"Top subjects with multiple diagnoses:\")\n",
    "    sorted_subjects = sorted(multi_diagnosis_subjects.items(), key=lambda x: len(x[1]['files']), reverse=True)\n",
    "    for subj, data in sorted_subjects[:5]:\n",
    "        unique_labels = set(data['labels'])\n",
    "        label_names = ['CN' if l == 0 else 'AD' for l in unique_labels]\n",
    "        print(f\"  {subj}: {len(data['files'])} scans (diagnoses: {', '.join(label_names)})\")\n",
    "\n",
    "# Show subjects with multiple scans (same diagnosis)\n",
    "multi_scan_subjects = {subj: data for subj, data in subject_files.items() if len(data['files']) > 1}\n",
    "print(f\"Subjects with multiple scans: {len(multi_scan_subjects)}\")\n",
    "\n",
    "# Split subjects (not files) into train/val/test to prevent data leakage\n",
    "subject_ids = list(subject_files.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(subject_ids)\n",
    "\n",
    "# Split by subjects: 80% train, 10% val, 10% test\n",
    "total_subjects = len(subject_ids)\n",
    "train_subjects = int(0.85 * total_subjects)\n",
    "val_subjects = int(0.1 * total_subjects)\n",
    "\n",
    "train_subject_ids = subject_ids[:train_subjects]\n",
    "val_subject_ids = subject_ids[train_subjects:train_subjects + val_subjects]\n",
    "test_subject_ids = subject_ids[train_subjects + val_subjects:]\n",
    "\n",
    "print(f\"\\nSubject split (ensuring no data leakage):\")\n",
    "print(f\"  Train subjects: {len(train_subject_ids)}\")\n",
    "print(f\"  Val subjects: {len(val_subject_ids)}\")\n",
    "print(f\"  Test subjects: {len(test_subject_ids)}\")\n",
    "\n",
    "# Verify no overlap between splits\n",
    "train_set = set(train_subject_ids)\n",
    "val_set = set(val_subject_ids)\n",
    "test_set = set(test_subject_ids)\n",
    "overlaps = train_set.intersection(val_set) or train_set.intersection(test_set) or val_set.intersection(test_set)\n",
    "if overlaps:\n",
    "    print(f\"❌ ERROR: Data leakage detected! Overlapping subjects: {overlaps}\")\n",
    "else:\n",
    "    print(\"✓ No data leakage: All subject splits are mutually exclusive\")\n",
    "\n",
    "# Create file lists from subject splits\n",
    "def create_file_lists(subject_ids):\n",
    "    files = []\n",
    "    labels = []\n",
    "    for subject_id in subject_ids:\n",
    "        subject_data = subject_files[subject_id]\n",
    "        files.extend(subject_data['files'])\n",
    "        labels.extend(subject_data['labels'])  # Use actual labels for each file\n",
    "    return files, labels\n",
    "\n",
    "train_files, train_labels = create_file_lists(train_subject_ids)\n",
    "val_files, val_labels = create_file_lists(val_subject_ids)\n",
    "test_files, test_labels = create_file_lists(test_subject_ids)\n",
    "\n",
    "print(f\"\\nFile split:\")\n",
    "print(f\"  Train files: {len(train_files)} (AD: {sum(train_labels)}, CN: {len(train_labels) - sum(train_labels)})\")\n",
    "print(f\"  Val files: {len(val_files)} (AD: {sum(val_labels)}, CN: {len(val_labels) - sum(val_labels)})\")\n",
    "print(f\"  Test files: {len(test_files)} (AD: {sum(test_labels)}, CN: {len(test_labels) - sum(test_labels)})\")\n",
    "\n",
    "# Process and save the datasets\n",
    "if len(train_files) > 0:\n",
    "    print(\"\\nProcessing datasets...\")\n",
    "    process_and_save(train_files, train_labels, train_path / \"ad_cn_train.pkl\")\n",
    "    process_and_save(val_files, val_labels, val_path / \"ad_cn_val.pkl\")\n",
    "    process_and_save(test_files, test_labels, test_path / \"ad_cn_test.pkl\")\n",
    "    print(\"✓ All datasets have been processed and saved with no data leakage!\")\n",
    "else:\n",
    "    print(\"❌ No files to process. Please check your data organization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def visualize_processed_image(image_tensor, title=\"Processed Image\", slice_indices=None):\n",
    "    \"\"\"\n",
    "    Visualize slices of a processed 3D image tensor.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: 3D numpy array or PyTorch tensor of shape (channels, height, width, depth)\n",
    "        title: Title for the plot\n",
    "        slice_indices: List of slice indices to show for each view. If None, uses middle slices.\n",
    "    \"\"\"\n",
    "    # Convert to numpy if it's a tensor\n",
    "    if hasattr(image_tensor, 'numpy'):\n",
    "        image_data = image_tensor.numpy()\n",
    "    else:\n",
    "        image_data = image_tensor\n",
    "    \n",
    "    # Remove channel dimension if present (should be shape (1, H, W, D))\n",
    "    if image_data.ndim == 4:\n",
    "        image_data = image_data[0]  # Remove channel dimension\n",
    "    \n",
    "    # Get dimensions\n",
    "    h, w, d = image_data.shape\n",
    "    \n",
    "    # Set default slice indices if not provided\n",
    "    if slice_indices is None:\n",
    "        slice_indices = [h//2, w//2, d//2]  # Middle slices\n",
    "    \n",
    "    # Create figure with subplots for three views\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    gs = gridspec.GridSpec(1, 3, figure=fig)\n",
    "    \n",
    "    # Axial view (horizontal slice)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    axial_slice = image_data[slice_indices[0], :, :]\n",
    "    im1 = ax1.imshow(axial_slice, cmap='gray', aspect='equal')\n",
    "    ax1.set_title(f'Axial View (slice {slice_indices[0]})')\n",
    "    ax1.set_xlabel('Width')\n",
    "    ax1.set_ylabel('Depth')\n",
    "    plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Sagittal view (side-to-side slice)\n",
    "    ax2 = fig.add_subplot(gs[1])\n",
    "    sagittal_slice = image_data[:, slice_indices[1], :]\n",
    "    im2 = ax2.imshow(sagittal_slice, cmap='gray', aspect='equal')\n",
    "    ax2.set_title(f'Sagittal View (slice {slice_indices[1]})')\n",
    "    ax2.set_xlabel('Depth')\n",
    "    ax2.set_ylabel('Height')\n",
    "    plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Coronal view (front-to-back slice)\n",
    "    ax3 = fig.add_subplot(gs[2])\n",
    "    coronal_slice = image_data[:, :, slice_indices[2]]\n",
    "    im3 = ax3.imshow(coronal_slice, cmap='gray', aspect='equal')\n",
    "    ax3.set_title(f'Coronal View (slice {slice_indices[2]})')\n",
    "    ax3.set_xlabel('Width')\n",
    "    ax3.set_ylabel('Height')\n",
    "    plt.colorbar(im3, ax=ax3, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.suptitle(f'{title}\\nShape: {image_data.shape}, Range: [{image_data.min():.3f}, {image_data.max():.3f}]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_and_visualize_dataset(dataset_path, num_samples=3):\n",
    "    \"\"\"\n",
    "    Load a processed dataset and visualize some sample images.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the pickle file containing processed data\n",
    "        num_samples: Number of sample images to visualize\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    images = data['images']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    print(f\"Dataset loaded:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Label distribution: AD={torch.sum(labels).item()}, CN={len(labels) - torch.sum(labels).item()}\")\n",
    "    print(f\"  Image data type: {images.dtype}\")\n",
    "    print(f\"  Image value range: [{images.min().item():.3f}, {images.max().item():.3f}]\")\n",
    "    \n",
    "    # Visualize a few sample images\n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        label_name = \"AD\" if labels[i].item() == 1 else \"CN\"\n",
    "        visualize_processed_image(\n",
    "            images[i], \n",
    "            title=f\"Sample {i+1} - {label_name} (label: {labels[i].item()})\"\n",
    "        )\n",
    "    \n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize processed images from the training set\n",
    "train_dataset_path = train_path / \"ad_cn_train.pkl\"\n",
    "\n",
    "if train_dataset_path.exists():\n",
    "    print(\"Loading and visualizing training dataset...\")\n",
    "    images, labels = load_and_visualize_dataset(train_dataset_path, num_samples=10)\n",
    "else:\n",
    "    print(f\"Training dataset not found at: {train_dataset_path}\")\n",
    "    print(\"Please run the data processing cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also visualize an original image before processing for comparison\n",
    "def visualize_original_vs_processed(original_path, processed_tensor, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compare an original image with its processed version.\n",
    "    \"\"\"\n",
    "    # Load original image\n",
    "    original_img = nib.load(original_path)\n",
    "    original_data = original_img.get_fdata()\n",
    "    \n",
    "    # Handle 4D images\n",
    "    if original_data.ndim == 4:\n",
    "        original_data = original_data[:, :, :, 0]\n",
    "    \n",
    "    # Get processed data\n",
    "    if hasattr(processed_tensor, 'numpy'):\n",
    "        processed_data = processed_tensor.numpy()\n",
    "    else:\n",
    "        processed_data = processed_tensor\n",
    "    \n",
    "    # Remove channel dimension if present\n",
    "    if processed_data.ndim == 4:\n",
    "        processed_data = processed_data[0]\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Original image slices\n",
    "    h_orig, w_orig, d_orig = original_data.shape\n",
    "    axes[0, 0].imshow(original_data[h_orig//2, :, :], cmap='gray')\n",
    "    axes[0, 0].set_title(f'Original - Axial (slice {h_orig//2})')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(original_data[:, w_orig//2, :], cmap='gray')\n",
    "    axes[0, 1].set_title(f'Original - Sagittal (slice {w_orig//2})')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(original_data[:, :, d_orig//2], cmap='gray')\n",
    "    axes[0, 2].set_title(f'Original - Coronal (slice {d_orig//2})')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Processed image slices\n",
    "    h_proc, w_proc, d_proc = processed_data.shape\n",
    "    axes[1, 0].imshow(processed_data[h_proc//2, :, :], cmap='gray')\n",
    "    axes[1, 0].set_title(f'Processed - Axial (slice {h_proc//2})')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(processed_data[:, w_proc//2, :], cmap='gray')\n",
    "    axes[1, 1].set_title(f'Processed - Sagittal (slice {w_proc//2})')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(processed_data[:, :, d_proc//2], cmap='gray')\n",
    "    axes[1, 2].set_title(f'Processed - Coronal (slice {d_proc//2})')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix}\\nOriginal: {original_data.shape} [{original_data.min():.3f}, {original_data.max():.3f}] → '\n",
    "                f'Processed: {processed_data.shape} [{processed_data.min():.3f}, {processed_data.max():.3f}]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show comparison for one sample\n",
    "if train_dataset_path.exists():\n",
    "    print(\"\\nComparing original vs processed image...\")\n",
    "    \n",
    "    # Get the first training file path\n",
    "    first_file = train_files[0]\n",
    "    print(f\"Original file: {first_file}\")\n",
    "    \n",
    "    # Load the processed version\n",
    "    with open(train_dataset_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    processed_image = data['images'][0]\n",
    "    \n",
    "    visualize_original_vs_processed(first_file, processed_image, \"Original vs Processed Comparison\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
