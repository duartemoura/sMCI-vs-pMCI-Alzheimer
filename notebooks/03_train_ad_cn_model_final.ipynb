{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NII9mdA2nsgf"
      },
      "source": [
        "# 3. Train AD/CN Classification Model with Hyperparameter Tuning\n",
        "\n",
        "This enhanced version includes systematic hyperparameter tuning using Optuna, similar to the Keras Tuner approach in the original implementation. The model architecture and key hyperparameters are optimized automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNSeN88sByPe"
      },
      "source": [
        "### Inputs and Outputs\n",
        "\n",
        "**Inputs:**\n",
        "- `ad_cn_train.pkl`, `ad_cn_val.pkl`: Pickled dictionaries containing preprocessed training and validation data.\n",
        "\n",
        "**Outputs:**\n",
        "- `hyperparameter_study.pkl`: A joblib file containing the results of the Optuna hyperparameter search.\n",
        "- `ad_cn_model_best_tuned.pth`: The PyTorch state dictionary for the best performing model.\n",
        "- **W&B Artifacts:**\n",
        "  - Logs of hyperparameters and metrics for each tuning trial.\n",
        "  - Logs for the final model training run, including loss/accuracy curves.\n",
        "  - The final trained model file (`.pth`) saved as a W&B Artifact.\n",
        "- Visualization images of data augmentation saved to `reports/figures/augmentation_examples/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QSDlYKXQn9ZT"
      },
      "outputs": [],
      "source": [
        "%pip install optuna wandb monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3a95e115"
      },
      "outputs": [],
      "source": [
        "!pip install optuna-integration[pytorch_lightning]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSlNno1mnsgg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "import joblib\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsCRkPpKnwP6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfMh5lAmnsgg"
      },
      "source": [
        "### Define Paths and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3yuEBawnsgh"
      },
      "outputs": [],
      "source": [
        "drive_base_path = Path(\"PATH_TO_DATA\")\n",
        "drive_save_path = drive_base_path / \"model_outputs\" \n",
        "\n",
        "# Use the mounted Google Drive paths\n",
        "train_path = drive_base_path / \"data\" / \"fdg\"  / \"ad_cn_train.pkl\"\n",
        "val_path = drive_base_path / \"data\" / \"fdg\" / \"ad_cn_val.pkl\"\n",
        "\n",
        "# Create a directory for saving models within Google Drive (or a specified location)\n",
        "model_save_path = drive_save_path / \"saved_models\"\n",
        "\n",
        "model_save_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Create visualization directory for augmentation examples\n",
        "figures_path = Path(\"./reports/figures/augmentation_examples/\")\n",
        "figures_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# W&B Login\n",
        "#wandb.login()\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
        "N_TRIALS = 30  # Number of hyperparameter combinations to try\n",
        "EPOCHS_PER_TRIAL = 35  # Epochs for each trial\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "print(f\"Training on {DEVICE}\")\n",
        "print(f\"Will run {N_TRIALS} hyperparameter optimization trials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WcNhiE-nsgh"
      },
      "source": [
        "### Dataset and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GQn85CFnsgh"
      },
      "outputs": [],
      "source": [
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandAffine,\n",
        ")\n",
        "\n",
        "class ADNIDataset(Dataset):\n",
        "    def __init__(self, pkl_file):\n",
        "        # Load data once\n",
        "        with open(pkl_file, 'rb') as f:\n",
        "            data_dict = pickle.load(f)\n",
        "        self.images = data_dict[\"images\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        self.num_samples = len(self.images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Return raw tensor. Ensure it has channel dim (C, D, H, W)\n",
        "        # Assuming original data is (D, H, W), add channel dim:\n",
        "        if image.ndim == 3:\n",
        "            image = image.unsqueeze(0)\n",
        "        return image.float(), label.float()\n",
        "\n",
        "# Define GPU-based augmentations\n",
        "# prob=0.5 applies the transform 50% of the time\n",
        "gpu_augmentations = Compose([\n",
        "    # Rotation and Shift (Translation) combined in one affine matrix for speed\n",
        "    RandAffine(\n",
        "    prob=0.5,\n",
        "    rotate_range=(0.349, 0.349, 0.349),  # ~20 degrees in radians\n",
        "    translate_range=(10, 10, 10),        # Shift pixels\n",
        "    padding_mode=\"zeros\",\n",
        "    device=DEVICE\n",
        "    ),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-vYGiBInsgi"
      },
      "source": [
        "### Tunable Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvWAAJS8nsgi"
      },
      "outputs": [],
      "source": [
        "class TunableCNN3D(nn.Module):\n",
        "    def __init__(self, trial):\n",
        "        super(TunableCNN3D, self).__init__()\n",
        "\n",
        "        # Hyperparameters to tune\n",
        "        n_layers = trial.suggest_int('n_layers', 4, 6)\n",
        "        base_filters = trial.suggest_int('base_filters', 8, 16)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
        "        dense_units = trial.suggest_int('dense_units', 256, 1024, step=256)\n",
        "\n",
        "        layers = []\n",
        "        in_channels = 1\n",
        "\n",
        "        # Build convolutional layers\n",
        "        for i in range(n_layers):\n",
        "            out_channels = base_filters * (2 ** i)\n",
        "\n",
        "            if i == 0:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels)\n",
        "                ])\n",
        "            else:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels),\n",
        "                    nn.Dropout(dropout_rate)\n",
        "                ])\n",
        "\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels, dense_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(dense_units, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbheWe26nsgi"
      },
      "source": [
        "### Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwb2DXJjU90e"
      },
      "source": [
        "Optimize for validation loss in the optuna training optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5iDc9amnsgi"
      },
      "outputs": [],
      "source": [
        "# Load data globally ONCE to avoid reloading from Drive for every trial\n",
        "print(\"Loading datasets into memory for Optuna...\")\n",
        "train_dataset = ADNIDataset(train_path)\n",
        "val_dataset = ADNIDataset(val_path)\n",
        "print(\"Datasets loaded.\")\n",
        "\n",
        "def objective(trial):\n",
        "    # W&B Init for each trial\n",
        "    # run = wandb.init(\n",
        "    #     project=\"AD_CN_FDG\",\n",
        "    #     group=\"Hyperparameter-Tuning\",\n",
        "    #     config=trial.params,\n",
        "    #     reinit=True\n",
        "    # )\n",
        "\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    #weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # Create model\n",
        "    model = TunableCNN3D(trial).to(DEVICE)\n",
        "\n",
        "    # Data loaders\n",
        "    # Use the pre-loaded global datasets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "    # Training setup\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) #weight_decay=weight_decay\n",
        "    criterion = nn.BCELoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                 mode='min',  # Use 'min' for loss\n",
        "                                                 factor=0.5,\n",
        "                                                 patience=3,\n",
        "                                                 )\n",
        "\n",
        "\n",
        "    best_val_loss = float('inf') # Initialize to infinity for loss minimization\n",
        "    patience = 7 # Patience for early stopping in trials\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS_PER_TRIAL):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to GPU as early as possible\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # Apply GPU augmentations\n",
        "            # RandAffine expects (C, spatial...), so we apply it to each image in the batch\n",
        "            images = torch.stack([gpu_augmentations(img) for img in images])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)                 # (N,1) probs (0-1) due to Sigmoid in model\n",
        "            labels  = labels.view(-1, 1).float()    # (N,1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # FIX: outputs are already probabilities, do not apply sigmoid again\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                # Move data to GPU as early as possible\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                predicted = (outputs.squeeze() > 0.5).float()\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct_val / total_val\n",
        "\n",
        "        # Print epoch loss and accuracy for verbosity\n",
        "        print(f\"Trial {trial.number}, Epoch {epoch+1}/{EPOCHS_PER_TRIAL}, Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}\")\n",
        "\n",
        "        # Log metrics to W&B\n",
        "        # wandb.log({\n",
        "        #     \"epoch\": epoch + 1,\n",
        "        #     \"train_loss\": epoch_train_loss,\n",
        "        #     \"train_accuracy\": epoch_train_accuracy,\n",
        "        #     \"val_loss\": epoch_val_loss,\n",
        "        #     \"val_accuracy\": epoch_val_accuracy\n",
        "        # })\n",
        "\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Report intermediate result for pruning (report loss instead of accuracy)\n",
        "        trial.report(epoch_val_loss, epoch)\n",
        "\n",
        "        # Handle pruning\n",
        "        if trial.should_prune():\n",
        "            # wandb.finish()\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        # Early stopping check for trials (check loss instead of accuracy)\n",
        "        if epoch_val_loss < best_val_loss: # Change to minimize loss\n",
        "            best_val_loss = epoch_val_loss # Update best loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # FIX: Add break statement to enforce early stopping\n",
        "        if epochs_no_improve == patience:\n",
        "            print(f\"Trial {trial.number}: Early stopping after {epoch+1} epochs.\")\n",
        "            break\n",
        "\n",
        "    # At the end of the function, before returning\n",
        "    # wandb.finish()\n",
        "    return best_val_loss # Return loss instead of accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pTwskXEnsgj"
      },
      "source": [
        "### Run Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg-PY1Bynsgj"
      },
      "outputs": [],
      "source": [
        "# Create study\n",
        "study = optuna.create_study(\n",
        "    direction='minimize', # Changed to minimize for loss optimization\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=8, n_warmup_steps=5) # Removed pruner for small number of trials\n",
        ")\n",
        "\n",
        "# Define a callback to save the study whenever a new best trial is found\n",
        "def save_best_study_callback(study, trial):\n",
        "    if study.best_trial.number == trial.number:\n",
        "        joblib.dump(study, model_save_path / \"hyperparameter_study.pkl\")\n",
        "        print(f\"New best trial found! Study saved to {model_save_path / 'hyperparameter_study.pkl'}\")\n",
        "\n",
        "# Run optimization\n",
        "study.optimize(objective, n_trials=N_TRIALS, callbacks=[save_best_study_callback])\n",
        "\n",
        "# Print results\n",
        "print(\"\\nOptimization completed!\")\n",
        "print(f\"Best trial: {study.best_trial.number}\")\n",
        "print(f\"Best validation loss: {study.best_value:.4f}\")\n",
        "print(\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save study\n",
        "joblib.dump(study, model_save_path / \"hyperparameter_study.pkl\")\n",
        "print(f\"\\nStudy saved to {model_save_path / 'hyperparameter_study.pkl'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdN2iEQ4nsgj"
      },
      "source": [
        "### Train Final Model with Best Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zVA72PkXEC9"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Load the Optuna study\n",
        "study = joblib.load(model_save_path / \"hyperparameter_study.pkl\")\n",
        "\n",
        "# Verify it loaded\n",
        "print(f\"Best trial number: {study.best_trial.number}\")\n",
        "print(f\"Best validation loss: {study.best_value:.4f}\")\n",
        "print(\"Best params:\", study.best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8hi9Ojonsgj"
      },
      "outputs": [],
      "source": [
        "# Create final model with best hyperparameters\n",
        "print(\"\\nTraining final model...\")\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# --- Parameter Configuration ---\n",
        "# Define manual parameters here to override the Optuna study results if desired.\n",
        "# Set manual_params to None to use the best parameters found by Optuna.\n",
        "manual_params = None\n",
        "# Example of how to define manual parameters:\n",
        "# manual_params = {\n",
        "#     'n_layers': 5,\n",
        "#     'base_filters': 15,\n",
        "#     'dropout_rate': 0.30103587368703894,\n",
        "#     'dense_units': 1024,\n",
        "#     'lr': 0.0008165831666899182,\n",
        "# }\n",
        "\n",
        "# Determine which parameters to use\n",
        "if manual_params is not None:\n",
        "    print(\"Using MANUAL hyperparameters.\")\n",
        "    final_params = manual_params\n",
        "elif 'study' in globals() and study is not None:\n",
        "    print(\"Using OPTUNA BEST hyperparameters.\")\n",
        "    final_params = study.best_params\n",
        "else:\n",
        "    raise ValueError(\"No hyperparameters available. Please define 'manual_params' or run the Optuna study.\")\n",
        "\n",
        "print(f\"Final Parameters: {final_params}\")\n",
        "\n",
        "# Start a new W&B run for the final model\n",
        "final_run = wandb.init(\n",
        "    project=\"AD_CN_Classification\",\n",
        "    name=\"Final-Model-Training\",\n",
        "    job_type=\"train\",\n",
        "    config=final_params\n",
        ")\n",
        "\n",
        "# Create a mock trial with best parameters\n",
        "class MockTrial:\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "\n",
        "    def suggest_int(self, name, low, high, step=1):\n",
        "        return self.params[name]\n",
        "\n",
        "    def suggest_float(self, name, low, high, log=False):\n",
        "        return self.params[name]\n",
        "\n",
        "# Assuming 'study' object is available from the previous cell execution\n",
        "mock_trial = MockTrial(final_params)\n",
        "final_model = TunableCNN3D(mock_trial).to(DEVICE)\n",
        "\n",
        "# Training setup\n",
        "# Load data lazily in the dataset\n",
        "train_dataset = ADNIDataset(train_path)\n",
        "val_dataset = ADNIDataset(val_path)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    final_model.parameters(),\n",
        "    lr=final_params['lr'],\n",
        "    weight_decay=final_params.get('weight_decay', 0)\n",
        ")\n",
        "# Using BCELoss because the model ends with nn.Sigmoid()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Extended training for final model\n",
        "FINAL_EPOCHS = 100\n",
        "best_val_loss = float('inf')\n",
        "patience = 10  # Number of epochs to wait for improvement\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Lists to store metrics for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_aucs = []\n",
        "val_aucs = []\n",
        "\n",
        "for epoch in range(FINAL_EPOCHS):\n",
        "    # Training\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    train_probs_all = []\n",
        "    train_targets_all = []\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{FINAL_EPOCHS} [Train]\"):\n",
        "        # Move data to GPU as early as possible\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Apply GPU augmentations\n",
        "        images = torch.stack([gpu_augmentations(img) for img in images])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = final_model(images)\n",
        "\n",
        "        # Reshape for BCELoss: outputs (B, 1) -> (B,), labels (B,) -> (B,)\n",
        "        # Or keep both (B, 1). Let's standardise to flat vectors.\n",
        "        outputs_flat = outputs.view(-1)\n",
        "        labels_flat = labels.view(-1).float()\n",
        "\n",
        "        loss = criterion(outputs_flat, labels_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Model outputs are ALREADY probabilities (0-1). Do not apply sigmoid.\n",
        "        predicted = (outputs_flat > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels_flat).sum().item()\n",
        "\n",
        "        train_probs_all.extend(outputs_flat.detach().cpu().numpy())\n",
        "        train_targets_all.extend(labels_flat.cpu().numpy())\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_train_accuracy = correct_train / total_train\n",
        "\n",
        "    try:\n",
        "        epoch_train_auc = roc_auc_score(train_targets_all, train_probs_all)\n",
        "    except ValueError:\n",
        "        epoch_train_auc = 0.5\n",
        "\n",
        "    # Store training metrics\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "    train_aucs.append(epoch_train_auc)\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    final_model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    val_probs_all = []\n",
        "    val_targets_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = final_model(images)\n",
        "\n",
        "            outputs_flat = outputs.view(-1)\n",
        "            labels_flat = labels.view(-1).float()\n",
        "\n",
        "            loss = criterion(outputs_flat, labels_flat)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            predicted = (outputs_flat > 0.5).float()\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels_flat).sum().item()\n",
        "\n",
        "            val_probs_all.extend(outputs_flat.cpu().numpy())\n",
        "            val_targets_all.extend(labels_flat.cpu().numpy())\n",
        "\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    epoch_val_accuracy = correct_val / total_val\n",
        "\n",
        "    try:\n",
        "        epoch_val_auc = roc_auc_score(val_targets_all, val_probs_all)\n",
        "    except ValueError:\n",
        "        epoch_val_auc = 0.5\n",
        "\n",
        "    # Store validation metrics\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "    val_aucs.append(epoch_val_auc)\n",
        "\n",
        "\n",
        "    # Print epoch loss and accuracy for verbosity\n",
        "    print(f\"Epoch {epoch+1}/{FINAL_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, Train AUC: {epoch_train_auc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}, Val AUC: {epoch_val_auc:.4f}\")\n",
        "\n",
        "    # Log metrics to W&B\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": epoch_train_loss,\n",
        "        \"train_accuracy\": epoch_train_accuracy,\n",
        "        \"train_auc\": epoch_train_auc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": epoch_val_accuracy,\n",
        "        \"val_auc\": epoch_val_auc,\n",
        "        \"best_val_loss\": best_val_loss\n",
        "    })\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(final_model.state_dict(), model_save_path / \"ad_cn_model_best_tuned.pth\")\n",
        "        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
        "        epochs_no_improve = 0 # Reset counter\n",
        "        wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
        "        wandb.run.summary[\"best_epoch\"] = epoch + 1\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    # Early stopping check\n",
        "    if epochs_no_improve == patience:\n",
        "        print(f\"Early stopping after {epoch+1} epochs due to no improvement in validation loss for {patience} epochs.\")\n",
        "        break # Stop training loop\n",
        "\n",
        "print(\"\\nFinal training completed!\")\n",
        "\n",
        "# Log model artifact to W&B\n",
        "model_artifact = wandb.Artifact(\n",
        "    name=\"ad_cn_classifier\",\n",
        "    type=\"model\",\n",
        "    description=\"Best 3D CNN model for AD/CN classification after hyperparameter tuning.\"\n",
        ")\n",
        "model_artifact.add_file(model_save_path / \"ad_cn_model_best_tuned.pth\")\n",
        "final_run.log_artifact(model_artifact)\n",
        "\n",
        "print(f\"Best model saved to {model_save_path / 'ad_cn_model_best_tuned.pth'}\")\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKnfjRmlBpq"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8316465c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import torch\n",
        "\n",
        "# Plotting metrics\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(train_aucs, label='Train AUC')\n",
        "plt.plot(val_aucs, label='Validation AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.title('Training and Validation AUC')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- ROC Curve for Best Model ---\n",
        "print(\"\\nEvaluating best model for ROC Curve...\")\n",
        "\n",
        "# Load the best model weights\n",
        "best_model_path = model_save_path / \"ad_cn_model_best_tuned.pth\"\n",
        "final_model.load_state_dict(torch.load(best_model_path))\n",
        "final_model.eval()\n",
        "\n",
        "y_true = []\n",
        "y_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        outputs = final_model(images)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_scores.extend(outputs.view(-1).cpu().numpy())\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (Best Validation Model)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewTd3IdMk8cH"
      },
      "source": [
        "## Evaluate on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1e69d11"
      },
      "outputs": [],
      "source": [
        "# --- Evaluation on Test Dataset ---\n",
        "# Define the path to your test data (modify as needed)\n",
        "test_path = drive_base_path / \"data\" / \"fdg\" / \"ad_cn_test.pkl\" # UPDATE THIS PATH\n",
        "\n",
        "if test_path.exists():\n",
        "    print(\"\\nEvaluating the best model on the test dataset...\")\n",
        "\n",
        "    # Load the test dataset lazily\n",
        "    test_dataset = ADNIDataset(test_path)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
        "\n",
        "    # Load the best trained model\n",
        "    best_model_path = model_save_path / \"ad_cn_model_best_tuned.pth\"\n",
        "    if best_model_path.exists():\n",
        "        # Create a mock trial with best parameters to instantiate the model architecture\n",
        "        class MockTrial:\n",
        "            def __init__(self, params):\n",
        "                self.params = params\n",
        "\n",
        "            def suggest_int(self, name, low, high, step=1):\n",
        "                return self.params[name]\n",
        "\n",
        "            def suggest_float(self, name, low, high, log=False):\n",
        "                return self.params[name]\n",
        "\n",
        "        # Determine params to rebuild the architecture\n",
        "        # Use 'final_params' from the training cell if available, else fallback to study\n",
        "        if 'final_params' in globals():\n",
        "            print(\"Using 'final_params' from training session to rebuild model.\")\n",
        "            rebuild_params = final_params\n",
        "        elif 'study' in globals() and study is not None:\n",
        "            print(\"Using 'study.best_params' to rebuild model.\")\n",
        "            rebuild_params = study.best_params\n",
        "        else:\n",
        "             # Fallback error\n",
        "             raise ValueError(\"Cannot rebuild model: 'final_params' and 'study' are missing. Please define architecture parameters.\")\n",
        "\n",
        "        mock_trial = MockTrial(rebuild_params)\n",
        "        test_model = TunableCNN3D(mock_trial).to(DEVICE)\n",
        "        test_model.load_state_dict(torch.load(best_model_path))\n",
        "        test_model.eval() # Set the model to evaluation mode\n",
        "\n",
        "        criterion = nn.BCELoss() # Use BCELoss to match training\n",
        "\n",
        "        test_loss = 0.0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        with torch.no_grad(): # No gradient calculation during evaluation\n",
        "            for images, labels in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                outputs = test_model(images)\n",
        "\n",
        "                outputs_flat = outputs.view(-1)\n",
        "                labels_flat = labels.view(-1).float()\n",
        "\n",
        "                loss = criterion(outputs_flat, labels_flat)\n",
        "                test_loss += loss.item() * images.size(0)\n",
        "\n",
        "                # outputs are probabilities (0-1)\n",
        "                predicted = (outputs_flat > 0.5).float()\n",
        "                total_test += labels.size(0)\n",
        "                correct_test += (predicted == labels_flat).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_accuracy = correct_test / total_test\n",
        "\n",
        "        print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Best model file not found at {best_model_path}. Please ensure the final training completed successfully.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Test data file not found at {test_path}. Please update the path or ensure the file exists.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "O7OGnYwLByPh"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
