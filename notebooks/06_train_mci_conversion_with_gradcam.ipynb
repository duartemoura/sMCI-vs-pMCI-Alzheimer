{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nsNvy-IVox4"
      },
      "source": [
        "# 6. Train MCI Conversion Model with Transfer Learning\n",
        "\n",
        "This notebook trains 3D CNN models to predict MCI to AD conversion using transfer learning from the pre-trained AD/CN model. The training is performed using 5-fold cross-validation for robust evaluation.\n",
        "\n",
        "Key steps:\n",
        "1. Load the pre-trained AD/CN model as a starting point\n",
        "2. Fine-tune the model on each fold of the MCI conversion data\n",
        "3. Save the best model from each fold for later uncertainty analysis\n",
        "4. Track training metrics across all folds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFGXKry0YizT"
      },
      "source": [
        "### Inputs and Outputs\n",
        "\n",
        "**Inputs:**\n",
        "- K-fold data splits (`train_fold_*.pkl`, `val_fold_*.pkl`) from notebook 05.\n",
        "- The pre-trained AD/CN classifier model (`ad_cn_model_best_tuned.pth`) from notebook 03.\n",
        "- The hyperparameter study file (`hyperparameter_study.pkl`) to define the model architecture.\n",
        "\n",
        "**Outputs:**\n",
        "- The best performing model for each of the 5 folds (`mci_model_fold_*_best.pth`).\n",
        "- `training_summary.csv` with performance metrics for each fold.\n",
        "- **W&B Artifacts:**\n",
        "  - A run for each fold, logging metrics and saving the best model as an artifact.\n",
        "  - A final summary run logging the cross-validation results table and plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8rkgHPKtTBM"
      },
      "outputs": [],
      "source": [
        "%pip install optuna monai captum nibabel \"numpy>=2.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrMMHFCUVox6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from scipy import ndimage\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    RandAffine,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OaVARS4WSXU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emBOfuo2Vox8"
      },
      "source": [
        "### Define Paths and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7W06vUWIVox8"
      },
      "outputs": [],
      "source": [
        "# Paths tau\n",
        "# kfold_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/data/tau/kfold/\")\n",
        "# pretrained_model_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/ad_cn_model_best_tuned.pth\")\n",
        "# output_model_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion_tau/\")\n",
        "# C_LEARNING_RATE = 0.004391419283753976\n",
        "# FT_LEARNING_RATE = C_LEARNING_RATE/20\n",
        "# DROPOUT = 0.284169899466295\n",
        "\n",
        "# Paths fdg\n",
        "kfold_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/data/fdg/kfold/\")\n",
        "pretrained_model_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/ad_cn_model_best_tuned.pth\")\n",
        "output_model_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion/\")\n",
        "C_LEARNING_RATE = 0.004391419283753976\n",
        "FT_LEARNING_RATE = C_LEARNING_RATE/20\n",
        "DROPOUT = 0.284169899466295\n",
        "\n",
        "output_model_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Training parameters\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_FOLDS = 5\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 8  # Smaller batch size for MCI data\n",
        "\n",
        "\n",
        "print(f\"Training on {DEVICE}\")\n",
        "print(f\"Will train {NUM_FOLDS} models for {EPOCHS} epochs each\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC__OFuhVox-"
      },
      "source": [
        "### Dataset and Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KRWr5EFVox-"
      },
      "outputs": [],
      "source": [
        "class ADNIDataset(Dataset):\n",
        "    def __init__(self, pkl_file):\n",
        "        # Load data once\n",
        "        with open(pkl_file, 'rb') as f:\n",
        "            data_dict = pickle.load(f)\n",
        "        self.images = data_dict[\"images\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        self.num_samples = len(self.images)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Return raw tensor. Ensure it has channel dim (C, D, H, W)\n",
        "        # Assuming original data is (D, H, W), add channel dim:\n",
        "        if image.ndim == 3:\n",
        "            image = image.unsqueeze(0)\n",
        "        return image.float(), label.float()\n",
        "\n",
        "# Define GPU-based augmentations\n",
        "# prob=0.5 applies the transform 50% of the time\n",
        "gpu_augmentations = Compose([\n",
        "    # Rotation and Shift (Translation) combined in one affine matrix for speed\n",
        "    RandAffine(\n",
        "    prob=0.5,\n",
        "    rotate_range=(0.349, 0.349, 0.349),  # ~20 degrees in radians\n",
        "    translate_range=(10, 10, 10),        # Shift pixels\n",
        "    padding_mode=\"zeros\",\n",
        "    device=DEVICE\n",
        "    ),\n",
        "])\n",
        "\n",
        "class TunableCNN3D(nn.Module):\n",
        "    def __init__(self, n_layers, base_filters, dropout_rate, dense_units):\n",
        "        super(TunableCNN3D, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_channels = 1\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            out_channels = base_filters * (2 ** i)\n",
        "            if i == 0:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels)\n",
        "                ])\n",
        "            else:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels),\n",
        "                    nn.Dropout(dropout_rate)\n",
        "                ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels, dense_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(dense_units, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnGI655bkysd"
      },
      "source": [
        "### Model Explainability with Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxuJhHQHkysd"
      },
      "outputs": [],
      "source": [
        "from captum.attr import LayerGradCam, LayerAttribution\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.ndimage import gaussian_filter, binary_opening, binary_closing\n",
        "\n",
        "class LogitWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps the model to return logits (pre-sigmoid) for Grad-CAM stability.\n",
        "    \"\"\"\n",
        "    def __init__(self, m):\n",
        "        super().__init__()\n",
        "        self.m = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.m.features(x)\n",
        "        # Execute classifier layers except the last one (Sigmoid)\n",
        "        for i in range(len(self.m.classifier) - 1):\n",
        "            x = self.m.classifier[i](x)\n",
        "        return x  # logits (N, 1)\n",
        "\n",
        "def compute_3d_gradcam(model, input_tensor, target_class=1, layer_index=-3):\n",
        "    \"\"\"\n",
        "    Computes Grad-CAM using logits and earlier layers.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        input_tensor: (1, C, D, H, W)\n",
        "        target_class: 1 for Converter evidence, 0 for Non-Converter evidence\n",
        "        layer_index: Index of Conv3d layer to use (negative indexing recommended)\n",
        "    \"\"\"\n",
        "    # 1. Use wrapper to get logits\n",
        "    logit_model = LogitWrapper(model).eval()\n",
        "\n",
        "    # 2. Pick the correct target Conv3d layer\n",
        "    convs = [m for m in model.features.modules() if isinstance(m, nn.Conv3d)]\n",
        "    if len(convs) >= abs(layer_index):\n",
        "        target_layer = convs[layer_index]\n",
        "    else:\n",
        "        target_layer = convs[-1]\n",
        "        print(f\"Warning: Requested layer index {layer_index} out of bounds. Using last conv layer.\")\n",
        "\n",
        "    # 3. Grad-CAM with correct target (scalar output => target=0)\n",
        "    lgc = LayerGradCam(logit_model, target_layer)\n",
        "\n",
        "    # relu_attributions=False gets us the raw contribution\n",
        "    attr_raw = lgc.attribute(input_tensor, target=0, relu_attributions=False)\n",
        "\n",
        "    if target_class == 1:\n",
        "        # Evidence FOR class 1 (positive contribution to logit)\n",
        "        attr = torch.relu(attr_raw)\n",
        "    else:\n",
        "        # Evidence FOR class 0 (negative contribution to logit)\n",
        "        # A negative contribution to the logit decreases the probability of class 1,\n",
        "        # effectively acting as evidence for class 0.\n",
        "        attr = torch.relu(-attr_raw)\n",
        "\n",
        "    # 4. Upsample correctly (trilinear)\n",
        "    # Input shape to interpolate must be (N, C, D, H, W)\n",
        "    attr = F.interpolate(attr, size=input_tensor.shape[2:], mode=\"trilinear\", align_corners=False)\n",
        "\n",
        "    # Convert to numpy\n",
        "    h = attr.squeeze().detach().cpu().numpy()\n",
        "\n",
        "    return h\n",
        "\n",
        "def get_vis_settings(modality):\n",
        "    \"\"\"Returns visualization settings for specific modalities.\"\"\"\n",
        "    presets = {\n",
        "        'tau': {'mask_thr': 0.08, 'cam_thr_p': 90, 'sigma': 0.8, 'alpha': 0.45},\n",
        "        'fdg': {'mask_thr': 0.08, 'cam_thr_p': 95, 'sigma': 0.6, 'alpha': 0.35}\n",
        "    }\n",
        "    return presets.get(modality, presets['tau'])\n",
        "\n",
        "def process_cam_image(img, hmap, settings):\n",
        "    \"\"\"\n",
        "    Helper to process image and CAM for visualization/export.\n",
        "    Returns: imgw (normalized image), hmap_norm (normalized CAM), brain_mask\n",
        "    \"\"\"\n",
        "    # 1. Image Normalization\n",
        "    p1, p99 = np.percentile(img, (1, 99))\n",
        "    imgw = np.clip(img, p1, p99)\n",
        "    imgw = (imgw - p1) / (p99 - p1 + 1e-8)\n",
        "\n",
        "    # 2. Brain Masking\n",
        "    brain_mask = imgw > settings['mask_thr']\n",
        "    brain_mask = binary_closing(binary_opening(brain_mask, iterations=1), iterations=2)\n",
        "    brain_mask = np.ones_like(imgw, dtype=bool)\n",
        "\n",
        "    # 3. Prepare CAM (Rectify -> Smooth -> Mask -> Threshold)\n",
        "    hmap_rect = np.maximum(hmap, 0)\n",
        "    hmap_rect = gaussian_filter(hmap_rect, sigma=settings['sigma'])\n",
        "    hmap_rect[~brain_mask] = 0\n",
        "\n",
        "    # Threshold CAM\n",
        "    vals = hmap_rect[brain_mask]\n",
        "    if vals.size > 0:\n",
        "        thr = np.percentile(vals, settings['cam_thr_p'])\n",
        "        hmap_rect[hmap_rect < thr] = 0\n",
        "\n",
        "    # 4. Normalize using ONLY masked voxels\n",
        "    vals = hmap_rect[brain_mask]\n",
        "    cap = np.percentile(vals, 99) if vals.size > 0 else (np.max(hmap_rect) + 1e-8)\n",
        "    hmap_norm = np.clip(hmap_rect, 0, cap) / (cap + 1e-8)\n",
        "\n",
        "    return imgw, hmap_norm, brain_mask\n",
        "\n",
        "def plot_explainability(image_tensor, heatmap, title=\"Grad-CAM\", save_path=None, modality='tau'):\n",
        "    \"\"\"\n",
        "    Plots improved visualization using modality-specific presets.\n",
        "    \"\"\"\n",
        "    # Get settings\n",
        "    settings = get_vis_settings(modality)\n",
        "\n",
        "    # Prepare data\n",
        "    img = image_tensor.squeeze().cpu().numpy() if torch.is_tensor(image_tensor) else image_tensor.squeeze()\n",
        "    hmap = heatmap.squeeze()\n",
        "\n",
        "    # Process\n",
        "    imgw, hmap_norm, brain_mask = process_cam_image(img, hmap, settings)\n",
        "\n",
        "    # 5. Show the most relevant slice\n",
        "    slice_idx = int(np.argmax(hmap_norm.sum(axis=(1, 2))))\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle(f\"{title} (Modality: {modality}, Best Slice: {slice_idx})\", fontsize=14)\n",
        "\n",
        "    # Axial (Best Slice)\n",
        "    axes[0].imshow(imgw[slice_idx, :, :], cmap='gray')\n",
        "    axes[0].imshow(hmap_norm[slice_idx, :, :], cmap='hot', alpha=settings['alpha'])\n",
        "    axes[0].set_title(f'Axial (Slice {slice_idx})')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Sagittal (Middle)\n",
        "    sag_slice = img.shape[2] // 2\n",
        "    axes[1].imshow(imgw[:, :, sag_slice], cmap='gray')\n",
        "    axes[1].imshow(hmap_norm[:, :, sag_slice], cmap='hot', alpha=settings['alpha'])\n",
        "    axes[1].set_title(f'Sagittal (Slice {sag_slice})')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Coronal (Middle)\n",
        "    cor_slice = img.shape[1] // 2\n",
        "    axes[2].imshow(imgw[:, cor_slice, :], cmap='gray')\n",
        "    axes[2].imshow(hmap_norm[:, cor_slice, :], cmap='hot', alpha=settings['alpha'])\n",
        "    axes[2].set_title(f'Coronal (Slice {cor_slice})')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path is not None:\n",
        "        save_path = Path(save_path)\n",
        "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Figure saved to: {save_path}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQwhT6AbVoyA"
      },
      "source": [
        "### Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuMhWIgBVoyB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, accuracy_score, balanced_accuracy_score\n",
        "\n",
        "def set_bn_eval(m):\n",
        "    \"\"\"\n",
        "    Sets BatchNorm layers to eval mode.\n",
        "    This prevents the running mean/var from being updated and\n",
        "    uses the robust pre-trained statistics instead of the noisy batch statistics.\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('BatchNorm') != -1:\n",
        "        m.eval()\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, criterion, device, freeze_bn=False):\n",
        "    model.train()\n",
        "    if freeze_bn:\n",
        "        model.apply(set_bn_eval)\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # Move data to GPU as early as possible\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Apply GPU augmentations\n",
        "        # RandAffine expects (C, spatial...), so we apply it to each image in the batch\n",
        "        images = torch.stack([gpu_augmentations(img) for img in images])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.squeeze(1), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    return running_loss / len(train_loader.dataset)\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Handle batch size 1 for batch normalization\n",
        "            if images.size(0) == 1:\n",
        "                # Temporarily set model to eval mode within the loop\n",
        "                # This is a workaround for batchnorm with batch_size=1\n",
        "                original_training_mode = model.training\n",
        "                model.eval()\n",
        "                outputs = model(images)\n",
        "                model.train(original_training_mode) # Restore original training mode\n",
        "            else:\n",
        "                outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs.squeeze(1), labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            all_predictions.extend(outputs.squeeze(1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss = running_loss / len(val_loader.dataset)\n",
        "\n",
        "    # Calculate metrics\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "    binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "\n",
        "    auc = roc_auc_score(all_labels, all_predictions)\n",
        "    accuracy = accuracy_score(all_labels, binary_predictions)\n",
        "    balanced_acc = balanced_accuracy_score(all_labels, binary_predictions)\n",
        "\n",
        "    return val_loss, auc, accuracy, balanced_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1neH-yfRVoyC"
      },
      "source": [
        "### K-Fold Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BTSKqYNkysd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBEEaWh2VoyF"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "study = joblib.load(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/hyperparameter_study.pkl\")\n",
        "\n",
        "# Extract best hyperparameters\n",
        "best_params = study.best_trial.params\n",
        "OPTIMAL_LR = best_params[\"lr\"]\n",
        "#print(f\"Using optimal learning rate from study: {OPTIMAL_LR}\")\n",
        "\n",
        "# Store results for all folds\n",
        "fold_results = []\n",
        "all_training_logs = []  # Changed: List to store full history\n",
        "\n",
        "for fold in range(1, NUM_FOLDS + 1):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training Fold {fold}/{NUM_FOLDS}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Load data for this fold\n",
        "    train_dataset = ADNIDataset(kfold_path / f\"train_fold_{fold}.pkl\")\n",
        "    val_dataset = ADNIDataset(kfold_path / f\"val_fold_{fold}.pkl\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Instantiate the exact same model architecture\n",
        "    model = TunableCNN3D(\n",
        "        n_layers=best_params[\"n_layers\"],\n",
        "        base_filters=best_params[\"base_filters\"],\n",
        "        dropout_rate=DROPOUT,\n",
        "        dense_units=best_params[\"dense_units\"]\n",
        "    ).to(DEVICE)\n",
        "\n",
        "\n",
        "    # Load pre-trained AD/CN model if available\n",
        "    if pretrained_model_path.exists():\n",
        "        print(\"Loading pre-trained AD/CN model...\")\n",
        "        model.load_state_dict(torch.load(pretrained_model_path))\n",
        "    else:\n",
        "        print(\"Pre-trained model not found. Training from scratch.\")\n",
        "\n",
        "    # --- RE-INITIALIZE CLASSIFIER HEAD ---\n",
        "    # The task is different (MCI conversion vs AD/CN), so starting from\n",
        "    # random weights for the classifier helps avoid getting stuck.\n",
        "    print(\"Re-initializing classifier head...\")\n",
        "    for layer in model.classifier:\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "            if layer.bias is not None:\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    # --- STAGE 1: FREEZE FEATURE EXTRACTOR AND TRAIN CLASSIFIER ---\n",
        "    print(\"Stage 1: Training the classifier head only...\")\n",
        "\n",
        "    # Freeze convolutional layers\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Optimizer for the classifier ONLY\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=C_LEARNING_RATE)\n",
        "    criterion = nn.BCELoss()  # Standard BCE Loss\n",
        "\n",
        "    # Train for a few epochs to warm up the new classifier\n",
        "    WARMUP_EPOCHS = 20\n",
        "    for epoch in range(WARMUP_EPOCHS):\n",
        "        # Freeze BN even in stage 1\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE, freeze_bn=True)\n",
        "        # Optionally run validation here for logging\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            val_loss, val_auc, val_accuracy, val_bacc = validate_epoch(model, val_loader, criterion, DEVICE)\n",
        "            print(f\"  Warm-up Epoch {epoch+1}/{WARMUP_EPOCHS}, Train Loss: {train_loss:.4f}, Val AUC: {val_auc:.4f}, Val B-Acc: {val_bacc:.4f}\")\n",
        "\n",
        "    # --- STAGE 2: UNFREEZE ALL LAYERS AND FINE-TUNE ---\n",
        "    print(\"Stage 2: Fine-tuning the entire model...\")\n",
        "\n",
        "    # Unfreeze all layers\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # Re-create the optimizer for the whole model with a VERY LOW learning rate\n",
        "    optimizer = optim.Adam(model.parameters(), lr=FT_LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, factor=0.5)  # Note: mode='max' for AUC\n",
        "\n",
        "    # Training tracking\n",
        "    best_val_auc = 0.0\n",
        "    best_epoch = 0\n",
        "    fold_history = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Freeze BN during fine-tuning (Crucial!)\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, DEVICE, freeze_bn=True)\n",
        "        val_loss, val_auc, val_accuracy, val_bacc = validate_epoch(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "        scheduler.step(val_auc)  # Use AUC for scheduler\n",
        "\n",
        "        # Save epoch results\n",
        "        epoch_results = {\n",
        "            'fold': fold,\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'val_auc': val_auc,\n",
        "            'val_accuracy': val_accuracy,\n",
        "            'val_balanced_accuracy': val_bacc\n",
        "        }\n",
        "        fold_history.append(epoch_results)\n",
        "\n",
        "        # Save best model\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(model.state_dict(), output_model_path / f\"mci_model_fold_{fold}_best.pth\")\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                  f\"Val AUC: {val_auc:.4f}, Val Acc: {val_accuracy:.4f}, Val B-Acc: {val_bacc:.4f}\")\n",
        "\n",
        "    # Add fold history to global logs\n",
        "    all_training_logs.extend(fold_history)\n",
        "\n",
        "    # Store fold results\n",
        "    fold_summary = {\n",
        "        'fold': fold,\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_val_auc': best_val_auc,\n",
        "        'final_train_loss': train_loss,\n",
        "        'final_val_loss': val_loss,\n",
        "        'final_val_accuracy': val_accuracy,\n",
        "        'final_val_balanced_accuracy': val_bacc\n",
        "    }\n",
        "    fold_results.append(fold_summary)\n",
        "\n",
        "    print(f\"Fold {fold} completed. Best AUC: {best_val_auc:.4f} at epoch {best_epoch}\")\n",
        "\n",
        "# Save detailed training history (per epoch)\n",
        "history_df = pd.DataFrame(all_training_logs)\n",
        "history_df.to_csv(output_model_path / \"training_history_full.csv\", index=False)\n",
        "print(f\"Detailed training history saved to {output_model_path / 'training_history_full.csv'}\")\n",
        "\n",
        "# Save fold summary\n",
        "results_df = pd.DataFrame(fold_results)\n",
        "results_df.to_csv(output_model_path / \"training_summary.csv\", index=False)\n",
        "\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETED\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"\\nFold Summary:\")\n",
        "print(results_df)\n",
        "print(f\"\\nMean AUC across folds: {results_df['best_val_auc'].mean():.4f} ± {results_df['best_val_auc'].std():.4f}\")\n",
        "print(f\"Mean Accuracy across folds: {results_df['final_val_accuracy'].mean():.4f} ± {results_df['final_val_accuracy'].std():.4f}\")\n",
        "print(f\"Mean Balanced Acc across folds: {results_df['final_val_balanced_accuracy'].mean():.4f} ± {results_df['final_val_balanced_accuracy'].std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7adKz-AFkyse"
      },
      "source": [
        "### Example: Visualizing Model Explanations with Grad-CAM\n",
        "\n",
        "After training, you can use Grad-CAM to visualize what regions of the brain the model focuses on when making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isTcG5hEkyse"
      },
      "outputs": [],
      "source": [
        "# Example: Generate Grad-CAM visualizations for a sample from validation set\n",
        "# Run this cell after training is complete\n",
        "import joblib\n",
        "import warnings\n",
        "import nibabel as nib\n",
        "from google.colab import files\n",
        "\n",
        "# --- SET MODALITY PRESET HERE ---\n",
        "MODALITY = 'fdg'  # Options: 'tau' or 'fdg'\n",
        "# --------------------------------\n",
        "\n",
        "# Define paths\n",
        "study_path = \"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/hyperparameter_study.pkl\"\n",
        "fold_to_analyze = 4 if MODALITY == 'tau' else 5\n",
        "model_path = output_model_path / f\"mci_model_fold_{fold_to_analyze}_best.pth\"\n",
        "explainability_output_path = output_model_path / \"gradcam_visualizations\"\n",
        "explainability_output_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Try to load parameters from the study file\n",
        "try:\n",
        "    # Load hyperparameters to make this cell standalone\n",
        "    study = joblib.load(study_path)\n",
        "    best_params = study.best_trial.params\n",
        "    print(\"Successfully loaded parameters from study file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load study file: {e}\")\n",
        "    print(\"Using manual fallback parameters.\")\n",
        "    best_params = {\n",
        "        \"n_layers\": 3,\n",
        "        \"base_filters\": 16,\n",
        "        \"dropout_rate\": 0.2,\n",
        "        \"dense_units\": 64,\n",
        "    }\n",
        "\n",
        "# Load the model architecture\n",
        "model = TunableCNN3D(\n",
        "    n_layers=best_params[\"n_layers\"],\n",
        "    base_filters=best_params[\"base_filters\"],\n",
        "    dropout_rate=best_params[\"dropout_rate\"],\n",
        "    dense_units=best_params[\"dense_units\"]\n",
        ").to(DEVICE)\n",
        "\n",
        "# Load trained weights\n",
        "if model_path.exists():\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Loaded model weights from {model_path}\")\n",
        "else:\n",
        "    print(f\"Warning: Model path {model_path} does not exist. Using random weights.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Get validation loader (NO AUGMENTATION)\n",
        "val_dataset = ADNIDataset(kfold_path / f\"val_fold_{fold_to_analyze}.pkl\")\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "print(\"Searching for a correctly predicted 'Converter' case (True Positive)...\")\n",
        "\n",
        "found_sample = False\n",
        "max_samples_to_check = 200  # Avoid infinite loops\n",
        "\n",
        "for i, (images, labels) in enumerate(val_loader):\n",
        "    if i >= max_samples_to_check:\n",
        "        print(f\"Checked {max_samples_to_check} samples without finding a match.\")\n",
        "        break\n",
        "\n",
        "    input_img = images[0:1].to(DEVICE)\n",
        "    true_label = labels[0].item()\n",
        "\n",
        "    # We only want True Label = 1 (Converter)\n",
        "    if true_label != 1:\n",
        "        continue\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        wrapper = LogitWrapper(model)\n",
        "        logit = wrapper(input_img).item()\n",
        "        prob = torch.sigmoid(torch.tensor(logit)).item()\n",
        "\n",
        "    # Check if correctly predicted as Converter (Prob > 0.8)\n",
        "    if prob > 0.7:\n",
        "        found_sample = True\n",
        "        print(f\"Found match at sample index {i}!\")\n",
        "        print(f\"True Label: {true_label} (Converter)\")\n",
        "        print(f\"Model Prediction (Prob): {prob:.4f}\")\n",
        "        break\n",
        "\n",
        "if found_sample:\n",
        "    predicted_class = 'Converter'\n",
        "    true_class = 'Converter'\n",
        "\n",
        "    # Compute Grad-CAM heatmap\n",
        "    # Explain the predicted class (Converter => 1)\n",
        "    heatmap = compute_3d_gradcam(model, input_img, target_class=1, layer_index=-3)\n",
        "\n",
        "    # Create filename\n",
        "    filename = f\"gradcam_fold{fold_to_analyze}_TRUE_POSITIVE_prob{prob:.3f}_{MODALITY}.png\"\n",
        "    save_path = explainability_output_path / filename\n",
        "\n",
        "    # Visualize (uses centralized logic via modality arg)\n",
        "    plot_explainability(\n",
        "        input_img,\n",
        "        heatmap,\n",
        "        title=f\"Grad-CAM (Logits, Layer -3): Correct Converter Prediction\",\n",
        "        save_path=save_path,\n",
        "        modality=MODALITY\n",
        "    )\n",
        "\n",
        "    # --- NIfTI EXPORT FOR 3D SLICER ---\n",
        "    print(\"\\nPreparing NIfTI files for 3D Slicer...\")\n",
        "\n",
        "    # Prepare Numpy arrays\n",
        "    img_np = input_img.squeeze().detach().cpu().numpy()\n",
        "    h_np = heatmap.squeeze() # Raw unsmoothed map\n",
        "\n",
        "    # Use centralized processing to ensure consistency with the plot\n",
        "    settings = get_vis_settings(MODALITY)\n",
        "    _, h_norm, _ = process_cam_image(img_np, h_np, settings)\n",
        "\n",
        "    # Create NIfTI files\n",
        "    # Using identity affine since spacing is generic\n",
        "    affine = np.eye(4)\n",
        "\n",
        "    # Save raw PET image\n",
        "    pet_nii = nib.Nifti1Image(img_np.astype(np.float32), affine)\n",
        "    # Save processed (smoothed/masked) Heatmap\n",
        "    cam_nii = nib.Nifti1Image(h_norm.astype(np.float32), affine)\n",
        "\n",
        "    # Save locally\n",
        "    nib.save(pet_nii, \"pet_image.nii.gz\")\n",
        "    nib.save(cam_nii, \"gradcam_heatmap.nii.gz\")\n",
        "\n",
        "    # (Optional) Save a thresholded CAM mask\n",
        "    h_thr = (h_norm > 0.4).astype(np.float32)\n",
        "    nib.save(nib.Nifti1Image(h_thr, affine), \"gradcam_mask.nii.gz\")\n",
        "\n",
        "    print(\"Files saved locally: pet_image.nii.gz, gradcam_heatmap.nii.gz, gradcam_mask.nii.gz\")\n",
        "\n",
        "    # Download files\n",
        "    # print(\"Triggering downloads...\")\n",
        "    # files.download(\"pet_image.nii.gz\")\n",
        "    # files.download(\"gradcam_heatmap.nii.gz\")\n",
        "    # files.download(\"gradcam_mask.nii.gz\")\n",
        "\n",
        "else:\n",
        "    print(\"Could not find a correctly predicted Converter case in the searched samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d18471c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "MODALITY='fdg'\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "OUTPUT_DIR = output_model_path / \"paper_figures\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "CONFIDENCE_THRESHOLD = 0.70  # Threshold for \"High Confidence\"\n",
        "NUM_SAMPLES = 10        # Number of samples to include in the combined figure\n",
        "MODALITY_SETTINGS = get_vis_settings(MODALITY)\n",
        "\n",
        "print(f\"Searching for {NUM_SAMPLES} high-confidence (>{CONFIDENCE_THRESHOLD}) Converter samples...\")\n",
        "\n",
        "# 1. Collect High Confidence Samples\n",
        "high_conf_samples = []\n",
        "model.eval()\n",
        "\n",
        "# Iterate through validation loader to find samples\n",
        "# Note: Assuming val_loader is already defined from previous cells with batch_size=1\n",
        "for i, (images, labels) in enumerate(val_loader):\n",
        "    if len(high_conf_samples) >= NUM_SAMPLES:\n",
        "        break\n",
        "\n",
        "    true_label = labels[0].item()\n",
        "    # We want True Positives (Converters)\n",
        "    if true_label != 1:\n",
        "        continue\n",
        "\n",
        "    input_img = images.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        wrapper = LogitWrapper(model)\n",
        "        logit = wrapper(input_img).item()\n",
        "        prob = torch.sigmoid(torch.tensor(logit)).item()\n",
        "\n",
        "    if prob > CONFIDENCE_THRESHOLD:\n",
        "        print(f\"Found Sample {len(high_conf_samples)+1}: Index {i}, Prob: {prob:.4f}\")\n",
        "\n",
        "        # Compute GradCAM\n",
        "        heatmap = compute_3d_gradcam(model, input_img, target_class=1, layer_index=-3)\n",
        "\n",
        "        # Prepare Data for Plotting\n",
        "        img_np = input_img.squeeze().cpu().numpy()\n",
        "        hmap_np = heatmap.squeeze()\n",
        "\n",
        "        # Process (Normalize, Mask, Smooth)\n",
        "        imgw, hmap_norm, brain_mask = process_cam_image(img_np, hmap_np, MODALITY_SETTINGS)\n",
        "\n",
        "        # Store data\n",
        "        high_conf_samples.append({\n",
        "            'idx': i,\n",
        "            'prob': prob,\n",
        "            'img': imgw,\n",
        "            'hmap': hmap_norm\n",
        "        })\n",
        "\n",
        "if not high_conf_samples:\n",
        "    print(\"No high confidence samples found. Try lowering the threshold.\")\n",
        "else:\n",
        "    print(f\"\\nProcessing {len(high_conf_samples)} samples for paper figures...\")\n",
        "\n",
        "# 2. Helper function to plot a single slice clean\n",
        "def plot_clean_slice(ax, img_slice, hmap_slice, alpha):\n",
        "    ax.imshow(img_slice, cmap='gray')\n",
        "    ax.imshow(hmap_slice, cmap='hot', alpha=alpha)\n",
        "    ax.axis('off')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "# 3. Generate Individual Figures and Per-Patient Strips\n",
        "alpha = MODALITY_SETTINGS['alpha']\n",
        "\n",
        "for sample_idx, sample in enumerate(high_conf_samples):\n",
        "    img = sample['img']\n",
        "    hmap = sample['hmap']\n",
        "    orig_idx = sample['idx']\n",
        "    prob = sample['prob']\n",
        "\n",
        "    # Use middle slices as requested\n",
        "    # Slice indices: (Depth, Height, Width) -> (Axial, Coronal, Sagittal) approx for standard orientation\n",
        "    slice_ax = img.shape[0] // 2\n",
        "    slice_cor = img.shape[1] // 2\n",
        "    slice_sag = img.shape[2] // 2\n",
        "\n",
        "    # --- A. Save Individual Slices (Axial, Sagittal, Coronal) ---\n",
        "    slices = {\n",
        "        'axial': (img[slice_ax, :, :], hmap[slice_ax, :, :]),\n",
        "        'coronal': (img[:, slice_cor, :], hmap[:, slice_cor, :]), # Note orientation usually needs flipping/rotation depending on lib\n",
        "        'sagittal': (img[:, :, slice_sag], hmap[:, :, slice_sag])\n",
        "    }\n",
        "\n",
        "    # Correcting orientation for visualization if necessary (often needed for Coronal/Sagittal in matplotlib)\n",
        "    # Assuming standard plotting: Sagittal and Coronal often need 90 deg rot or flip.\n",
        "    # Adjusting strictly for \"clean view\".\n",
        "\n",
        "    # Save each slice individually\n",
        "    for view_name, (img_s, hmap_s) in slices.items():\n",
        "        fig, ax = plt.subplots(figsize=(4, 4))\n",
        "        # Fix orientation for non-axial usually involves np.rot90\n",
        "        if view_name != 'axial':\n",
        "             img_s = np.rot90(img_s)\n",
        "             hmap_s = np.rot90(hmap_s)\n",
        "\n",
        "        plot_clean_slice(ax, img_s, hmap_s, alpha)\n",
        "\n",
        "        # Save without border\n",
        "        fname = OUTPUT_DIR / f\"patient_{orig_idx}_{view_name}.png\"\n",
        "        plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n",
        "        plt.margins(0,0)\n",
        "        plt.savefig(fname, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "        plt.close(fig)\n",
        "\n",
        "    # --- B. Save 3-Slice Strip WITH PROBABILITY (Next to each other) ---\n",
        "    # Using 4 columns: 3 for slices, 1 for text\n",
        "    # Width ratios favor the images (1) over the text (0.3)\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(14, 4), gridspec_kw={'width_ratios': [1, 1, 1, 0.3]})\n",
        "\n",
        "    # Axial\n",
        "    plot_clean_slice(axes[0], slices['axial'][0], slices['axial'][1], alpha)\n",
        "    # Sagittal (Middle in paper standard often, or Coronal)\n",
        "    plot_clean_slice(axes[1], np.rot90(slices['sagittal'][0]), np.rot90(slices['sagittal'][1]), alpha)\n",
        "    # Coronal\n",
        "    plot_clean_slice(axes[2], np.rot90(slices['coronal'][0]), np.rot90(slices['coronal'][1]), alpha)\n",
        "\n",
        "    # Probability Text\n",
        "    axes[3].axis('off')\n",
        "    axes[3].text(0.1, 0.5, f\"Prob:\\n{prob:.4f}\", va='center', ha='left', fontsize=16)\n",
        "\n",
        "    # CHANGE: wspace=0 to remove vertical separation lines\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    fname_strip = OUTPUT_DIR / f\"patient_{orig_idx}_combined_strip.png\"\n",
        "    plt.savefig(fname_strip, bbox_inches='tight', pad_inches=0, dpi=300)\n",
        "    plt.close(fig)\n",
        "\n",
        "print(f\"Individual figures and strips saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# 4. Generate Combined 4-Patient Figure (Grid)\n",
        "if len(high_conf_samples) > 0:\n",
        "    # Create a figure with N rows and 4 columns (3 slices + 1 text)\n",
        "    n_rows = len(high_conf_samples)\n",
        "    fig, axes = plt.subplots(n_rows, 4, figsize=(12, 3 * n_rows), gridspec_kw={'width_ratios': [1, 1, 1, 0.3]})\n",
        "\n",
        "    # Handle case if n_rows=1 where axes is 1D\n",
        "    if n_rows == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for i, sample in enumerate(high_conf_samples):\n",
        "        img = sample['img']\n",
        "        hmap = sample['hmap']\n",
        "        prob = sample['prob']\n",
        "\n",
        "        # Use middle slices as requested\n",
        "        slice_ax = img.shape[0] // 2\n",
        "        slice_cor = img.shape[1] // 2\n",
        "        slice_sag = img.shape[2] // 2\n",
        "\n",
        "        # Axial\n",
        "        plot_clean_slice(axes[i, 0], img[slice_ax, :, :], hmap[slice_ax, :, :], alpha)\n",
        "\n",
        "        # Sagittal\n",
        "        img_sag = np.rot90(img[:, :, slice_sag])\n",
        "        hmap_sag = np.rot90(hmap[:, :, slice_sag])\n",
        "        plot_clean_slice(axes[i, 1], img_sag, hmap_sag, alpha)\n",
        "\n",
        "        # Coronal\n",
        "        img_cor = np.rot90(img[:, slice_cor, :])\n",
        "        hmap_cor = np.rot90(hmap[:, slice_cor, :])\n",
        "        plot_clean_slice(axes[i, 2], img_cor, hmap_cor, alpha)\n",
        "\n",
        "        # Probability Text\n",
        "        axes[i, 3].axis('off')\n",
        "        axes[i, 3].text(0.1, 0.5, f\"Prob:\\n{prob:.4f}\", va='center', ha='left', fontsize=14)\n",
        "\n",
        "    # CHANGE: wspace=0 to remove vertical separation, hspace=0.05 for horizontal separation\n",
        "    plt.subplots_adjust(wspace=0, hspace=0.05)\n",
        "    fname_composite = OUTPUT_DIR / f\"composite_paper_figure_{MODALITY}.png\"\n",
        "    plt.savefig(fname_composite, bbox_inches='tight', dpi=300)\n",
        "    plt.show()\n",
        "    print(f\"Composite figure saved to {fname_composite}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf1hiOKofonM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import joblib # Explicitly import joblib for loading study file\n",
        "\n",
        "# Assuming `TunableCNN3D`, `ADNIDataset`, `get_vis_settings`, `process_cam_image`,\n",
        "# `LogitWrapper`, `compute_3d_gradcam` are defined in previous cells or globally available.\n",
        "\n",
        "def collect_high_conf_truepos_gradcams(\n",
        "    model,\n",
        "    loader,\n",
        "    device,\n",
        "    modality: str,\n",
        "    conf_thr: float = 0.70,\n",
        "    max_n: int = 40,\n",
        "    target_class: int = 1,\n",
        "    layer_index: int = -4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Collects PET volumes and Grad-CAM heatmaps for true-label converters (label==1)\n",
        "    with model probability > conf_thr.\n",
        "\n",
        "    Returns:\n",
        "        imgs:  list of (D,H,W) numpy arrays\n",
        "        cams:  list of (D,H,W) numpy arrays in [0,1] from your compute_3d_gradcam\n",
        "        probs: list of float probabilities\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    imgs, cams, probs = [], [], []\n",
        "    wrapper = LogitWrapper(model).to(device).eval()\n",
        "\n",
        "    # Get visualization settings for processing\n",
        "    settings = get_vis_settings(modality)\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=f\"Collecting {modality} high-conf TP\"):\n",
        "        # assumes batch_size=1\n",
        "        y = float(labels[0].item())\n",
        "        if y != 1.0:\n",
        "            continue\n",
        "\n",
        "        x = images.to(device)\n",
        "        with torch.no_grad():\n",
        "            logit = wrapper(x).item()\n",
        "            p = torch.sigmoid(torch.tensor(logit)).item()\n",
        "\n",
        "        if p < conf_thr:\n",
        "            continue\n",
        "\n",
        "        # 1. Compute Raw Grad-CAM (returns ONLY heatmap)\n",
        "        h_raw = compute_3d_gradcam(\n",
        "            model=model,\n",
        "            input_tensor=x,\n",
        "            target_class=target_class,\n",
        "            layer_index=layer_index\n",
        "        )\n",
        "\n",
        "        # 2. Process (Normalize, Mask, etc.) using the helper function\n",
        "        img_np = x.squeeze().cpu().numpy()\n",
        "        imgw, hmap_norm, _ = process_cam_image(img_np, h_raw, settings)\n",
        "\n",
        "        # imgw/hmap_norm are numpy (D,H,W)\n",
        "        imgs.append(imgw.astype(np.float32))\n",
        "        cams.append(hmap_norm.astype(np.float32))\n",
        "        probs.append(float(p))\n",
        "\n",
        "        if len(imgs) >= max_n:\n",
        "            break\n",
        "\n",
        "    return imgs, cams, probs\n",
        "\n",
        "\n",
        "def aggregate_maps(imgs, cams, agg=\"mean\"):\n",
        "    \"\"\"\n",
        "    Aggregate volumes across subjects: mean or median.\n",
        "    \"\"\"\n",
        "    imgs = np.stack(imgs, axis=0)   # (N,D,H,W)\n",
        "    cams = np.stack(cams, axis=0)   # (N,D,H,W)\n",
        "\n",
        "    if agg == \"median\":\n",
        "        img_agg = np.median(imgs, axis=0)\n",
        "        cam_agg = np.median(cams, axis=0)\n",
        "    else:\n",
        "        img_agg = np.mean(imgs, axis=0)\n",
        "        cam_agg = np.mean(cams, axis=0)\n",
        "\n",
        "    return img_agg, cam_agg\n",
        "\n",
        "\n",
        "# Load best hyperparameters to define model architecture\n",
        "# This assumes the hyperparameter_study.pkl exists and TunableCNN3D class is defined.\n",
        "study = joblib.load(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/hyperparameter_study.pkl\")\n",
        "best_params = study.best_trial.params\n",
        "\n",
        "# -------- RUN PER MODALITY (Aggregating Across All Folds) --------\n",
        "MODALITY = \"fdg\"  # <-- change to \"fdg\" then \"tau\" if needed elsewhere, but for cross-fold aggregation, we process one modality at a time.\n",
        "CONF_THR = 0.70\n",
        "MAX_N_PER_FOLD = 30  # Max number of samples to collect per fold\n",
        "AGG = \"mean\"        # or \"median\"\n",
        "LAYER_INDEX = -3    # keep consistent across modalities\n",
        "\n",
        "OUTPUT_DIR = output_model_path / \"thesis_figures\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Global lists to store results from all folds\n",
        "all_imgs = []\n",
        "all_cams = []\n",
        "all_probs = []\n",
        "\n",
        "print(f\"Aggregating Grad-CAMs for {MODALITY} across {NUM_FOLDS} folds...\")\n",
        "\n",
        "for fold in range(1, NUM_FOLDS + 1):\n",
        "    print(f\"Processing Fold {fold}...\")\n",
        "    # 1. Instantiate the model for the current fold using best hyperparameters\n",
        "    model = TunableCNN3D(\n",
        "        n_layers=best_params[\"n_layers\"],\n",
        "        base_filters=best_params[\"base_filters\"],\n",
        "        dropout_rate=best_params[\"dropout_rate\"], # Use dropout from best_params\n",
        "        dense_units=best_params[\"dense_units\"]\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # 2. Load the best trained weights for the current fold\n",
        "    model_fold_path = output_model_path / f\"mci_model_fold_{fold}_best.pth\"\n",
        "    if model_fold_path.exists():\n",
        "        model.load_state_dict(torch.load(model_fold_path))\n",
        "    else:\n",
        "        print(f\"Warning: Model for fold {fold} not found at {model_fold_path}. Skipping this fold.\")\n",
        "        continue # Skip to next fold if model not found\n",
        "\n",
        "    model.eval() # Ensure model is in evaluation mode for inference\n",
        "\n",
        "    # 3. Load the validation dataset for the current fold\n",
        "    val_dataset = ADNIDataset(kfold_path / f\"val_fold_{fold}.pkl\")\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # 4. Collect high-confidence true positives and their Grad-CAMs for this fold\n",
        "    fold_imgs, fold_cams, fold_probs = collect_high_conf_truepos_gradcams(\n",
        "        model=model,\n",
        "        loader=val_loader,\n",
        "        device=DEVICE,\n",
        "        modality=MODALITY,\n",
        "        conf_thr=CONF_THR,\n",
        "        max_n=MAX_N_PER_FOLD, # Apply MAX_N_PER_FOLD for each fold\n",
        "        layer_index=LAYER_INDEX\n",
        "    )\n",
        "\n",
        "    # 5. Extend global lists with results from the current fold\n",
        "    all_imgs.extend(fold_imgs)\n",
        "    all_cams.extend(fold_cams)\n",
        "    all_probs.extend(fold_probs)\n",
        "\n",
        "# After processing all folds, print summary and aggregate globally\n",
        "print(f\"\\n{MODALITY}: collected total N={len(all_imgs)} high-confidence true-positive samples \"\n",
        "      f\"(mean p={np.mean(all_probs):.3f}, min={np.min(all_probs):.3f}, max={np.max(all_probs):.3f})\")\n",
        "\n",
        "# 6. Aggregate across all collected samples from all folds\n",
        "img_mean, cam_mean = aggregate_maps(all_imgs, all_cams, agg=AGG)\n",
        "\n",
        "# Save the aggregated results for all folds\n",
        "np.savez_compressed(\n",
        "    OUTPUT_DIR / f\"agg_gradcam_{MODALITY}_all_folds.npz\", # Changed filename to reflect aggregation across all folds\n",
        "    img=img_mean,\n",
        "    cam=cam_mean,\n",
        "    probs=np.array(all_probs),\n",
        "    conf_thr=CONF_THR,\n",
        "    max_n_total_collected=len(all_imgs), # Save total number of samples collected across all folds\n",
        "    agg=AGG,\n",
        "    layer_index=LAYER_INDEX\n",
        ")\n",
        "\n",
        "print(\"Saved:\", OUTPUT_DIR / f\"agg_gradcam_{MODALITY}_all_folds.npz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-fEve2efp8B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as path_effects\n",
        "from pathlib import Path\n",
        "\n",
        "# --- PAPER QUALITY SETTINGS ---\n",
        "plt.rcParams.update({\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [\"Times New Roman\", \"DejaVu Serif\"],\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 12,\n",
        "    \"axes.labelsize\": 12\n",
        "})\n",
        "\n",
        "def robust_display(img, lo=1, hi=99):\n",
        "    vmin = np.percentile(img[img > 1e-4], lo) if np.any(img > 1e-4) else np.min(img)\n",
        "    vmax = np.percentile(img, hi)\n",
        "    img = np.clip(img, vmin, vmax)\n",
        "    norm_img = (img - vmin) / (vmax - vmin + 1e-8)\n",
        "    norm_img[img <= vmin] = 0\n",
        "    return norm_img\n",
        "\n",
        "def plot_aggregated_fdg_tau(\n",
        "    fdg_img, fdg_cam,\n",
        "    tau_img, tau_cam,\n",
        "    slices=None,\n",
        "    save_path=None,\n",
        "    axis=0,          # 0=Axial\n",
        "    cam_thr=0.0\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a high-quality, compact aggregated Grad-CAM figure (Paper Style).\n",
        "    \"\"\"\n",
        "    def get_slice(arr, idx, ax):\n",
        "        if ax == 0: return arr[idx, :, :]          # Axial\n",
        "        elif ax == 1: return np.rot90(arr[:, idx, :]) # Coronal\n",
        "        else: return np.rot90(arr[:, :, idx])      # Sagittal\n",
        "\n",
        "    if slices is None:\n",
        "        mid = fdg_img.shape[axis] // 2\n",
        "        slices = [mid - 12, mid - 6, mid, mid + 6, mid + 12]\n",
        "\n",
        "    fdg_bg = robust_display(fdg_img)\n",
        "    tau_bg = robust_display(tau_img)\n",
        "\n",
        "    ncols = len(slices)\n",
        "\n",
        "    # --- DYNAMIC FIGURE SIZE CALCULATION ---\n",
        "    # 1. Get aspect ratio of a single slice to ensure perfect fit\n",
        "    sample_slice = get_slice(fdg_img, slices[0], axis)\n",
        "    sh, sw = sample_slice.shape\n",
        "    img_aspect = sw / sh  # width / height\n",
        "\n",
        "    # 2. Define layout parameters\n",
        "    spacer_ratio = 0.05   # Spacer height relative to image height\n",
        "\n",
        "    # We have 2 rows of images + spacer.\n",
        "    # Total logical height units = 1 (FDG) + spacer + 1 (Tau)\n",
        "    total_img_h_units = 2 + spacer_ratio\n",
        "\n",
        "    # Total logical width units = ncols * width_per_image\n",
        "    # Since we set height unit = 1 (corresponding to image height 'sh'),\n",
        "    # width unit should be 'img_aspect' (corresponding to image width 'sw').\n",
        "    total_img_w_units = ncols * img_aspect\n",
        "\n",
        "    # Aspect ratio of the DATA part of the figure (Width / Height)\n",
        "    data_area_aspect = total_img_w_units / total_img_h_units\n",
        "\n",
        "    # 3. Set Figure Dimensions\n",
        "    # We fix the height of the image area, calculate width to match aspect,\n",
        "    # and add space for the colorbar at the bottom.\n",
        "    data_h_inches = 5.0\n",
        "    fig_w_inches = data_h_inches * data_area_aspect\n",
        "\n",
        "    # Reserve space for colorbar at bottom\n",
        "    bottom_margin_inches = 0.6\n",
        "    fig_h_inches = data_h_inches + bottom_margin_inches\n",
        "\n",
        "    # Calculate relative bottom margin for GridSpec\n",
        "    gs_bottom = bottom_margin_inches / fig_h_inches\n",
        "\n",
        "    fig = plt.figure(figsize=(fig_w_inches, fig_h_inches), dpi=300)\n",
        "\n",
        "    # Create GridSpec\n",
        "    # We use 'gs_bottom' to leave exact space for labels/colorbar\n",
        "    gs = fig.add_gridspec(\n",
        "        3, ncols,\n",
        "        height_ratios=[1, spacer_ratio, 1],   # FDG, Spacer, Tau\n",
        "        wspace=0,\n",
        "        hspace=0,\n",
        "        left=0, right=1, top=1, bottom=gs_bottom\n",
        "    )\n",
        "\n",
        "    axes = np.empty((2, ncols), dtype=object)\n",
        "\n",
        "    # FDG row\n",
        "    for j in range(ncols):\n",
        "        axes[0, j] = fig.add_subplot(gs[0, j])\n",
        "\n",
        "    # Tau row\n",
        "    for j in range(ncols):\n",
        "        axes[1, j] = fig.add_subplot(gs[2, j])\n",
        "\n",
        "\n",
        "    # Calculate relative slice positions for labeling\n",
        "    mid_idx = slices[len(slices)//2]\n",
        "\n",
        "    for j, s_idx in enumerate(slices):\n",
        "        # Slice label (e.g., \"Mid\", \"-6\", \"+12\")\n",
        "        offset = s_idx - mid_idx\n",
        "        if offset == 0:\n",
        "            lbl = \"Midline\"\n",
        "        else:\n",
        "            lbl = f\"{offset:+} mm\"\n",
        "\n",
        "        # --- FDG ROW ---\n",
        "        bg_s = get_slice(fdg_bg, s_idx, axis)\n",
        "        cam_s = get_slice(fdg_cam, s_idx, axis)\n",
        "        cam_s_masked = np.ma.masked_where(cam_s < cam_thr, cam_s)\n",
        "\n",
        "        # Force aspect='equal' just in case, though figsize should handle it\n",
        "        axes[0, j].imshow(bg_s, cmap=\"gray\", origin='lower', aspect='equal')\n",
        "        im = axes[0, j].imshow(cam_s_masked, cmap=\"hot\", alpha=0.6, vmin=cam_thr, vmax=1.0, origin='lower', aspect='equal')\n",
        "        axes[0, j].axis(\"off\")\n",
        "\n",
        "        # Add in-set text for slice info\n",
        "        axes[0, j].text(0.05, 0.95, lbl, transform=axes[0, j].transAxes,\n",
        "                        color='white', fontsize=10, va='top', ha='left', fontweight='bold',\n",
        "                        path_effects=[path_effects.withStroke(linewidth=2, foreground=\"black\")])\n",
        "\n",
        "        # --- TAU ROW ---\n",
        "        bg_s_tau = get_slice(tau_bg, s_idx, axis)\n",
        "        cam_s_tau = get_slice(tau_cam, s_idx, axis)\n",
        "        cam_s_tau_masked = np.ma.masked_where(cam_s_tau < cam_thr, cam_s_tau)\n",
        "\n",
        "        axes[1, j].imshow(bg_s_tau, cmap=\"gray\", origin='lower', aspect='equal')\n",
        "        axes[1, j].imshow(cam_s_tau_masked, cmap=\"hot\", alpha=0.6, vmin=cam_thr, vmax=1.0, origin='lower', aspect='equal')\n",
        "        axes[1, j].axis(\"off\")\n",
        "\n",
        "    # Row Labels (High contrast, rotated)\n",
        "    # Placed relative to the first axes of each row\n",
        "    axes[0, 0].text(-0.02, 0.5, \"FDG-PET\", transform=axes[0, 0].transAxes,\n",
        "                    va='center', ha='right', fontsize=14, fontweight='bold', rotation=90)\n",
        "    axes[1, 0].text(-0.02, 0.5, \"Tau-PET\", transform=axes[1, 0].transAxes,\n",
        "                    va='center', ha='right', fontsize=14, fontweight='bold', rotation=90)\n",
        "\n",
        "    # Refined Colorbar in the reserved bottom space\n",
        "    # Position: [left, bottom, width, height] in figure coordinates\n",
        "    # Centered horizontally, inside the bottom margin area\n",
        "    cbar_width = 0.4\n",
        "    cbar_height = 0.02\n",
        "    cbar_bottom = (gs_bottom - cbar_height) / 2 # Center vertically in the margin\n",
        "\n",
        "    cbar_ax = fig.add_axes([0.5 - cbar_width/2, cbar_bottom, cbar_width, cbar_height])\n",
        "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
        "    cbar.set_label(f\"Relevance Probability (Grad-CAM > {cam_thr})\", fontsize=11)\n",
        "    cbar.outline.set_visible(False)\n",
        "    cbar.ax.tick_params(size=0)\n",
        "\n",
        "    if save_path:\n",
        "        save_path = Path(save_path)\n",
        "        save_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "        # bbox_inches='tight' might mess up our perfect calculation,\n",
        "        # but usually it's safe if we used standard margins.\n",
        "        # Since we manually controlled everything, we can try saving without it or with it.\n",
        "        # 'tight' is safer for text labels extending out.\n",
        "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
        "        print(\"Saved figure to:\", save_path)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------- LOAD AND PLOT --------\n",
        "# Preserving your exact paths\n",
        "base_fdg_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion/thesis_figures\")\n",
        "base_tau_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion_tau/thesis_figures\")\n",
        "\n",
        "\n",
        "if (base_fdg_path / \"agg_gradcam_fdg.npz\").exists() and (base_tau_path / \"agg_gradcam_tau.npz\").exists():\n",
        "    fdg_npz = np.load(base_fdg_path / \"agg_gradcam_fdg.npz\")\n",
        "    tau_npz = np.load(base_tau_path / \"agg_gradcam_tau.npz\")\n",
        "\n",
        "    # Check shape to define sensible slices\n",
        "    D_dim = fdg_npz[\"img\"].shape[0]\n",
        "    mid = D_dim // 2\n",
        "    center_slices = [mid - 12, mid - 6, mid, mid + 6, mid + 12]\n",
        "\n",
        "    plot_aggregated_fdg_tau(\n",
        "        fdg_img=fdg_npz[\"img\"],\n",
        "        fdg_cam=fdg_npz[\"cam\"],\n",
        "        tau_img=tau_npz[\"img\"],\n",
        "        tau_cam=tau_npz[\"cam\"],\n",
        "        slices=center_slices,\n",
        "        save_path=base_fdg_path / \"FIG_agg_gradcam_fdg_vs_tau_coronal_PAPER.png\",\n",
        "        axis=1,          # 0 = AXIAL\n",
        "        cam_thr=0.25\n",
        "\n",
        "\n",
        "    )\n",
        "else:\n",
        "    print(\"Could not find .npz files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYnI0C-gQi-F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
