{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVuA0lgLytYT"
      },
      "source": [
        "# 7. Monte Carlo Dropout for Uncertainty Estimation\n",
        "\n",
        "This notebook applies Monte Carlo (MC) dropout to the trained MCI conversion models to estimate prediction uncertainty. By performing multiple forward passes with dropout enabled at inference time, we can obtain a distribution of predictions for each subject.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "1.  **Custom MCDropout Layer**: A custom dropout layer is defined that remains active during inference.\n",
        "2.  **Modified Model**: The trained 3D CNN models are loaded, and their standard dropout layers are replaced with the new `MCDropout` layers.\n",
        "3.  **Iterative Inference**: For each subject in each validation fold, the model performs 500 forward passes, generating a distribution of predictions.\n",
        "4.  **Uncertainty Calculation**: The mean and standard deviation of these prediction distributions are calculated. The standard deviation serves as a measure of the model's uncertainty for that prediction.\n",
        "5.  **Results Aggregation**: The results for all subjects across all folds (ground truth label, mean prediction, and standard deviation) are aggregated and saved into a single pandas DataFrame for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auQ3j08MytYU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "-0PNk2YR1n98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ_wdbHMytYU"
      },
      "source": [
        "### Define Paths and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KlZElAMVy1OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1AZLA2DytYU"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "kfold_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/data/tau/kfold\")\n",
        "ad_cn_model_dir = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/\")  # Path to the AD/CN model directory\n",
        "model_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion_tau/\")\n",
        "output_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion_tau/monte_carlo/\")\n",
        "output_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Create visualization directory\n",
        "figures_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/reports/figures/fdg/uncertainty_distributions/\")\n",
        "figures_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Parameters\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUM_FOLDS = 5\n",
        "MC_ITERATIONS = 500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE"
      ],
      "metadata": {
        "id": "emDWmrK2CIk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTJyeq5tytYU"
      },
      "source": [
        "### Custom Dataset and MCDropout Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqdCrqukytYU"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "class ADNIDataset(Dataset):\n",
        "    def __init__(self, pkl_file):\n",
        "        with open(pkl_file, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.images = data[\"images\"]\n",
        "        self.labels = data[\"labels\"]\n",
        "        self.subject_ids = data[\"subject_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx].unsqueeze(0), self.labels[idx].float(), self.subject_ids[idx]\n",
        "\n",
        "class MCDropout(nn.Dropout):\n",
        "    def forward(self, x):\n",
        "        # Enable dropout during eval mode for MC inference\n",
        "        return nn.functional.dropout(x, self.p, True, self.inplace)\n",
        "\n",
        "# This is the model architecture from notebook 03 and 06\n",
        "class TunableCNN3D(nn.Module):\n",
        "    def __init__(self, n_layers, base_filters, dropout_rate, dense_units):\n",
        "        super(TunableCNN3D, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_channels = 1\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            out_channels = base_filters * (2 ** i)\n",
        "            if i == 0:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels)\n",
        "                ])\n",
        "            else:\n",
        "                layers.extend([\n",
        "                    nn.Conv3d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool3d(2),\n",
        "                    nn.BatchNorm3d(out_channels),\n",
        "                    nn.Dropout(dropout_rate)  # We will replace this with MCDropout\n",
        "                ])\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool3d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_channels, dense_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),  # And this one\n",
        "            nn.Linear(dense_units, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def convert_to_mc_dropout(module, dropout_p):\n",
        "    \"\"\"Recursively replaces nn.Dropout with MCDropout.\"\"\"\n",
        "    for name, child in module.named_children():\n",
        "        if isinstance(child, nn.Dropout):\n",
        "            setattr(module, name, MCDropout(p=dropout_p))\n",
        "        else:\n",
        "            convert_to_mc_dropout(child, dropout_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6r-6_EytYV"
      },
      "source": [
        "### Perform Monte Carlo Dropout Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RCZ3HOBytYV"
      },
      "outputs": [],
      "source": [
        "all_results = []\n",
        "\n",
        "# Load the hyperparameter study to get the best model architecture\n",
        "try:\n",
        "    study = joblib.load(ad_cn_model_dir / \"hyperparameter_study.pkl\")\n",
        "    best_params = study.best_params\n",
        "    print(\"Successfully loaded best hyperparameters from study.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Could not find 'hyperparameter_study.pkl'. Cannot determine model architecture.\")\n",
        "    # You might want to hardcode the best params here as a fallback if the file is missing\n",
        "    best_params = None\n",
        "\n",
        "if best_params:\n",
        "    for i in range(1, NUM_FOLDS + 1):\n",
        "        print(f\"\\n--- Processing Fold {i} ---\")\n",
        "\n",
        "        # 1. Instantiate the model with the best architecture\n",
        "        model = TunableCNN3D(\n",
        "            n_layers=best_params['n_layers'],\n",
        "            base_filters=best_params['base_filters'],\n",
        "            dropout_rate=best_params['dropout_rate'],\n",
        "            dense_units=best_params['dense_units']\n",
        "        )\n",
        "\n",
        "        # 2. Convert to an MC Dropout model\n",
        "        convert_to_mc_dropout(model, dropout_p=best_params['dropout_rate'])\n",
        "        model.to(DEVICE)\n",
        "\n",
        "        # 3. Load the fine-tuned weights for the specific fold\n",
        "        model_file = model_path / f\"mci_model_fold_{i}_best.pth\"\n",
        "        if not model_file.exists():\n",
        "            print(f\"Model file not found for fold {i}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        model.load_state_dict(torch.load(model_file, map_location=DEVICE))\n",
        "        model.eval()  # Set to eval mode, but our MCDropout layers will still be active\n",
        "\n",
        "        # 4. Load data and perform inference\n",
        "        val_dataset = ADNIDataset(kfold_path / f\"val_fold_{i}.pkl\")\n",
        "        val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "        for image, label, subject_id in tqdm(val_loader, desc=f\"MC Dropout on Fold {i}\"):\n",
        "            # Fix: Remove the extra dimension at index 1\n",
        "            image = image.to(DEVICE).squeeze(1)\n",
        "            mc_predictions = []\n",
        "            for _ in range(MC_ITERATIONS):\n",
        "                with torch.no_grad():\n",
        "                    mc_predictions.append(model(image).item())\n",
        "\n",
        "            all_results.append({\n",
        "                \"subject_id\": subject_id[0],  # DataLoader wraps strings in a list\n",
        "                \"label\": label.item(),\n",
        "                \"mc_mean\": np.mean(mc_predictions),\n",
        "                \"mc_std\": np.std(mc_predictions)\n",
        "            })\n",
        "\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df.to_pickle(output_path / \"mci_mc_dropout_results.pkl\")\n",
        "    print(f\"\\nSaved all MC dropout results to {output_path / 'mci_mc_dropout_results.pkl'}\")\n",
        "else:\n",
        "    print(\"No results were generated. Please check model paths and data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5DMIYWAytYV"
      },
      "source": [
        "### Visualize Uncertainty Distributions\n",
        "\n",
        "Let's visualize the prediction distributions for a few select cases to better understand what the model's uncertainty looks like. We'll pick:\n",
        "1. A high-confidence correct pMCI prediction.\n",
        "2. A high-confidence correct sMCI prediction.\n",
        "3. A high-uncertainty (misclassified or borderline) case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCxSX09QytYV"
      },
      "outputs": [],
      "source": [
        "if 'results_df' in locals():\n",
        "    # Find interesting cases\n",
        "    correct_pmci = results_df[(results_df['label'] == 1) & (results_df['mc_mean'] > 0.8)].sort_values('mc_std', ascending=True)\n",
        "    correct_smci = results_df[(results_df['label'] == 0) & (results_df['mc_mean'] < 0.2)].sort_values('mc_std', ascending=True)\n",
        "    high_uncertainty = results_df.sort_values('mc_std', ascending=False)\n",
        "\n",
        "    cases = {\n",
        "        \"High-Confidence pMCI\": correct_pmci.iloc[0] if not correct_pmci.empty else None,\n",
        "        \"High-Confidence sMCI\": correct_smci.iloc[0] if not correct_smci.empty else None,\n",
        "        \"High-Uncertainty Case\": high_uncertainty.iloc[0] if not high_uncertainty.empty else None,\n",
        "    }\n",
        "\n",
        "    # This requires re-running inference for these specific subjects, which is slow.\n",
        "    # A simpler approach is to just plot a representative normal distribution from the saved mean/std.\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\n",
        "\n",
        "    for ax, (title, case) in zip(axes, cases.items()):\n",
        "        if case is None:\n",
        "            ax.set_title(f\"{title}\\n(No sample found)\")\n",
        "            ax.axis('off')\n",
        "            continue\n",
        "\n",
        "        # Generate data for a normal distribution plot\n",
        "        predictions = np.random.normal(case['mc_mean'], case['mc_std'], 1000)\n",
        "\n",
        "        sns.histplot(predictions, bins=30, kde=True, ax=ax)\n",
        "        ax.axvline(case['mc_mean'], color='r', linestyle='--', label=f\"Mean: {case['mc_mean']:.2f}\")\n",
        "        ax.axvline(0.5, color='k', linestyle=':', label='Threshold: 0.5')\n",
        "\n",
        "        true_label = \"pMCI\" if case['label'] == 1 else \"sMCI\"\n",
        "        ax.set_title(f\"{title}\\nSubject: {case['subject_id']}\\nTrue Label: {true_label} | Std Dev: {case['mc_std']:.3f}\")\n",
        "        ax.set_xlabel(\"Predicted Probability (pMCI)\")\n",
        "        ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_path / \"uncertainty_examples.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Results DataFrame not available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b1cef63"
      },
      "source": [
        "if 'results_df' in locals():\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Distribution of Mean Predictions\n",
        "    sns.histplot(data=results_df, x='mc_mean', hue='label', bins=30, kde=True, ax=axes[0])\n",
        "    axes[0].set_title('Distribution of Mean Predictions by True Label')\n",
        "    axes[0].set_xlabel('Mean Predicted Probability (pMCI)')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].axvline(0.5, color='k', linestyle=':', label='Threshold: 0.5')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Distribution of Standard Deviations (Uncertainty)\n",
        "    sns.histplot(data=results_df, x='mc_std', hue='label', bins=30, kde=True, ax=axes[1])\n",
        "    axes[1].set_title('Distribution of Prediction Uncertainty (Std Dev) by True Label')\n",
        "    axes[1].set_xlabel('Standard Deviation of Predictions')\n",
        "    axes[1].set_ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figures_path / \"overall_uncertainty_distributions_by_label.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Results DataFrame not available for overall visualization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58900231"
      },
      "source": [
        "if 'results_df' in locals():\n",
        "    print(\"Overall Summary Statistics:\")\n",
        "    display(results_df.groupby('label')[['mc_mean', 'mc_std']].agg(['mean', 'std']))\n",
        "\n",
        "    # You can also look at the overall distribution of standard deviations\n",
        "    print(\"\\nSummary Statistics of Standard Deviations:\")\n",
        "    display(results_df['mc_std'].describe())\n",
        "\n",
        "else:\n",
        "    print(\"Results DataFrame not available for summary statistics.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d584c657"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "if 'results_df' in locals():\n",
        "    # Calculate absolute prediction error (Distance from the True Label)\n",
        "    results_df['prediction_error'] = np.abs(results_df['label'] - results_df['mc_mean'])\n",
        "\n",
        "    # Correlation between Uncertainty (Std Dev) and Error\n",
        "    correlation = results_df[['mc_std', 'prediction_error']].corr().iloc[0, 1]\n",
        "    print(f\"Correlation between Uncertainty and Prediction Error: {correlation:.4f}\")\n",
        "\n",
        "    # Visualize relationship\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(data=results_df, x='mc_std', y='prediction_error', hue='label', alpha=0.6)\n",
        "    plt.title(f\"Prediction Error vs. Uncertainty (Correlation: {correlation:.2f})\")\n",
        "    plt.xlabel(\"Uncertainty (Standard Deviation)\")\n",
        "    plt.ylabel(\"Absolute Prediction Error |Label - Mean|\")\n",
        "    plt.legend(title='True Label')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"results_df is not defined. Please run the inference cells first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c3baa04"
      },
      "source": [
        "if 'results_df' in locals():\n",
        "    # Sort data by uncertainty (lowest uncertainty first)\n",
        "    sorted_df = results_df.sort_values('mc_std', ascending=True)\n",
        "\n",
        "    fractions = np.linspace(0.1, 1.0, 20)\n",
        "    accuracies = []\n",
        "    retained_ratios = []\n",
        "\n",
        "    for frac in fractions:\n",
        "        # Determine how many samples to keep (top % most certain)\n",
        "        n_samples = int(len(sorted_df) * frac)\n",
        "        if n_samples == 0: continue\n",
        "\n",
        "        # Select the subset of most confident predictions\n",
        "        subset = sorted_df.iloc[:n_samples]\n",
        "\n",
        "        # Calculate accuracy on this subset\n",
        "        # Prediction is 1 if mean probability > 0.5, else 0\n",
        "        preds = (subset['mc_mean'] > 0.5).astype(float)\n",
        "        acc = (preds == subset['label']).mean()\n",
        "\n",
        "        accuracies.append(acc)\n",
        "        retained_ratios.append(frac)\n",
        "\n",
        "    # Plot Accuracy vs. Retention\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(retained_ratios, accuracies, marker='o', linewidth=2)\n",
        "    plt.xlabel(\"Fraction of Data Retained (Low Uncertainty -> High Uncertainty)\")\n",
        "    plt.ylabel(\"Accuracy on Retained Data\")\n",
        "    plt.title(\"Accuracy vs. Uncertainty Retention Curve\")\n",
        "    plt.gca().invert_xaxis() # Invert x-axis to show effect of discarding uncertain samples (Right to Left)\n",
        "    plt.grid(True)\n",
        "    plt.annotate('Keeping only most\\ncertain samples', xy=(0.2, accuracies[1]), xytext=(0.4, accuracies[0]-0.05),\n",
        "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Interpretation:\\nIf the curve goes UP as you move to the LEFT (keeping fewer, more certain samples),\\nit means the uncertainty metric is working: the model is more accurate on cases where it is confident.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0cf46c"
      },
      "source": [
        "if 'results_df' in locals():\n",
        "    # 1. Identify Misclassifications (Threshold at 0.5)\n",
        "    results_df['predicted_label'] = (results_df['mc_mean'] > 0.5).astype(float)\n",
        "    results_df['is_correct'] = results_df['predicted_label'] == results_df['label']\n",
        "\n",
        "    # Filter for errors only\n",
        "    errors_df = results_df[~results_df['is_correct']].copy()\n",
        "\n",
        "    print(f\"Total Misclassified Subjects: {len(errors_df)} out of {len(results_df)}\")\n",
        "\n",
        "    # 2. Most Uncertain Errors\n",
        "    # These subjects correspond to the \"tail\" of the retention curve.\n",
        "    # Including them is what causes the accuracy to drop as you keep more data.\n",
        "    uncertain_errors = errors_df.sort_values('mc_std', ascending=False)\n",
        "\n",
        "    print(\"\\n--- Top 10 Most Uncertain Misclassifications (Likely causing the curve drop) ---\")\n",
        "    display(uncertain_errors.head(10)[['subject_id', 'label', 'mc_mean', 'mc_std', 'prediction_error']])\n",
        "\n",
        "    # 3. Most Confident Errors\n",
        "    # These are subjects the model got wrong despite being very consistent (low std).\n",
        "    # These lower the accuracy of even your \"best\" subset.\n",
        "    confident_errors = errors_df.sort_values('mc_std', ascending=True)\n",
        "\n",
        "    print(\"\\n--- Top 10 Most Confident Misclassifications (High confidence, wrong prediction) ---\")\n",
        "    display(confident_errors.head(10)[['subject_id', 'label', 'mc_mean', 'mc_std', 'prediction_error']])\n",
        "else:\n",
        "    print(\"results_df not found. Please run the inference step first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b16838e"
      },
      "source": [
        "if 'results_df' in locals():\n",
        "    # Strategy: Reject the top 20% most uncertain predictions\n",
        "    threshold = results_df['mc_std'].quantile(0.80)\n",
        "\n",
        "    # Split into Accepted (Low Uncertainty) and Rejected (High Uncertainty)\n",
        "    accepted_df = results_df[results_df['mc_std'] <= threshold]\n",
        "    rejected_df = results_df[results_df['mc_std'] > threshold]\n",
        "\n",
        "    # Calculate accuracies\n",
        "    acc_original = (results_df['predicted_label'] == results_df['label']).mean()\n",
        "    acc_accepted = (accepted_df['predicted_label'] == accepted_df['label']).mean()\n",
        "\n",
        "    print(f\"Uncertainty Threshold (80th percentile): {threshold:.4f}\")\n",
        "    print(f\"Original Accuracy (All {len(results_df)} subjects): {acc_original:.2%}\")\n",
        "    print(f\"Filtered Accuracy (Best {len(accepted_df)} subjects): {acc_accepted:.2%}\")\n",
        "    print(f\"\\n--- Subjects 'Removed' due to High Uncertainty ({len(rejected_df)} subjects) ---\")\n",
        "\n",
        "    # Show the list of subjects to remove, sorted by how uncertain they are\n",
        "    display(rejected_df.sort_values('mc_std', ascending=False)[['subject_id', 'label', 'mc_mean', 'mc_std', 'is_correct']].head(15))\n",
        "\n",
        "    print(\"\\nNote: 'is_correct' shows that many of these highly uncertain cases were indeed misclassified (False).\")\n",
        "else:\n",
        "    print(\"results_df is needed for this analysis.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "RQKq6ALypY8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load previously saved results if available to skip inference\n",
        "\n",
        "output_path = Path(\"/content/drive/MyDrive/Mestrado/TFM/new_pipeline/model_outputs/saved_models/mci_conversion_tau/monte_carlo/\")\n",
        "\n",
        "results_file = output_path / \"mci_mc_dropout_results.pkl\"\n",
        "\n",
        "if results_file.exists():\n",
        "    print(f\"Loading saved MC Dropout results from {results_file}...\")\n",
        "    results_df = pd.read_pickle(results_file)\n",
        "    print(f\"Successfully loaded {len(results_df)} subject predictions.\")\n",
        "    display(results_df.head())\n",
        "else:\n",
        "    print(f\"Results file not found at {results_file}. Please run the inference cell above to generate results.\")"
      ],
      "metadata": {
        "id": "uFT_eaJMHKpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "# --- Calculate Optimal Threshold ---\n",
        "if 'results_df' in locals():\n",
        "    y_true = results_df['label'].values\n",
        "    mc_mean = results_df['mc_mean'].values\n",
        "\n",
        "    thresholds = np.linspace(0, 1, 1001)\n",
        "    scores = []\n",
        "\n",
        "    for t in thresholds:\n",
        "        preds = (mc_mean >= t).astype(int)\n",
        "        scores.append(balanced_accuracy_score(y_true, preds))\n",
        "\n",
        "    best_idx = np.argmax(scores)\n",
        "    THRESH = thresholds[best_idx]\n",
        "    print(THRESH)\n",
        "    print(f\"Calculated Optimal Threshold (Balanced Accuracy): {THRESH:.4f} (Score: {scores[best_idx]:.4f})\")\n",
        "else:\n",
        "    THRESH = 0.31\n",
        "    print(f\"results_df not found. Using default threshold: {THRESH}\")\n",
        "\n",
        "# --- Prepare Data for Visualization ---\n",
        "df = results_df.copy()\n",
        "\n",
        "# SDpercentile in [0,100]; higher percentile = larger std = lower confidence\n",
        "df[\"sd_percentile\"] = df[\"mc_std\"].rank(pct=True) * 100.0\n",
        "\n",
        "# Confidence Score (CS): higher is better (lower std)\n",
        "df[\"CS\"] = 100.0 - df[\"sd_percentile\"]\n",
        "\n",
        "# Convenience: distance to threshold\n",
        "df[\"dist_to_thresh\"] = (df[\"mc_mean\"] - THRESH).abs()\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "ui4thYG2FnZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- assumes df has: mc_mean, mc_std and THRESH ---\n",
        "\n",
        "# Compute Confidence Score (paper definition) if missing\n",
        "if \"CS\" not in df.columns:\n",
        "    df = df.copy()\n",
        "    df[\"sd_percentile\"] = df[\"mc_std\"].rank(pct=True) * 100.0\n",
        "    df[\"CS\"] = 100.0 - df[\"sd_percentile\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# MAIN plot: CS vs mean score (show ALL points)\n",
        "sc = ax.scatter(\n",
        "    df[\"mc_mean\"],\n",
        "    df[\"CS\"],\n",
        "    c=df[\"mc_std\"],        # color by raw uncertainty (optional but informative)\n",
        "    s=22,\n",
        "    alpha=0.9)\n",
        "\n",
        "cb = fig.colorbar(sc, ax=ax)\n",
        "cb.set_label(\"MC standard deviation\")\n",
        "\n",
        "# Threshold\n",
        "ax.axvline(THRESH, linestyle=\"--\", linewidth=2)\n",
        "ax.text(THRESH - 0.02, 98, \"No Progress\", ha=\"right\", va=\"top\")\n",
        "ax.text(THRESH + 0.02, 98, \"Progress\", ha=\"left\", va=\"top\")\n",
        "\n",
        "# Left axis (CS)\n",
        "ax.set_xlabel(\"Model MCI-to-AD progression score (mean of MCD)\")\n",
        "ax.set_ylabel(\"Confidence Score (CS)\")\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 100)\n",
        "\n",
        "# SECONDARY axis: raw SD (same data, inverted like the paper)\n",
        "ax2 = ax.twinx()\n",
        "ax2.set_ylim(df[\"mc_std\"].min(), df[\"mc_std\"].max())\n",
        "ax2.invert_yaxis()\n",
        "ax2.set_ylabel(\"Standard Deviation\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5Z2QGwzvFtXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Inputs expected ----\n",
        "# df: DataFrame with subject_id, mc_mean, mc_std (CS optional)\n",
        "# THRESH: scalar decision threshold (e.g., 0.31)\n",
        "\n",
        "df2 = df.copy()\n",
        "\n",
        "# CS (paper-style) if missing\n",
        "if \"CS\" not in df2.columns:\n",
        "    df2[\"sd_percentile\"] = df2[\"mc_std\"].rank(pct=True) * 100.0\n",
        "    df2[\"CS\"] = 100.0 - df2[\"sd_percentile\"]\n",
        "\n",
        "df2[\"dist_to_thresh\"] = (df2[\"mc_mean\"] - THRESH).abs()\n",
        "\n",
        "# --- Select the same 4 archetypes (robust, consistent labeling) ---\n",
        "hi_cut = df2[\"CS\"].quantile(0.80)\n",
        "lo_cut = df2[\"CS\"].quantile(0.20)\n",
        "hi = df2[df2[\"CS\"] >= hi_cut]\n",
        "lo = df2[df2[\"CS\"] <= lo_cut]\n",
        "\n",
        "A = hi.nsmallest(1, \"mc_mean\").iloc[0]                 # far left, high confidence\n",
        "B = hi.nlargest(1, \"mc_mean\").iloc[0]                  # far right, high confidence\n",
        "C = hi.nsmallest(1, \"dist_to_thresh\").iloc[0]          # near threshold, high confidence\n",
        "D = lo.nsmallest(1, \"dist_to_thresh\").iloc[0]          # near threshold, low confidence\n",
        "\n",
        "picked = [(\"A\", A), (\"B\", B), (\"C\", C), (\"D\", D)]\n",
        "\n",
        "# ---- Fitted normal curves ----\n",
        "x = np.linspace(0, 1, 900)\n",
        "def normal_pdf(x, mu, sigma):\n",
        "    sigma = max(float(sigma), 1e-6)\n",
        "    return (1.0 / (sigma * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
        "\n",
        "plt.figure(figsize=(9.5, 4.8))\n",
        "\n",
        "for tag, row in picked:\n",
        "    mu = float(row[\"mc_mean\"])\n",
        "    sd = float(row[\"mc_std\"])\n",
        "    cs = float(row[\"CS\"])\n",
        "    sid = row[\"subject_id\"]\n",
        "\n",
        "    y = normal_pdf(x, mu, sd)\n",
        "    y = y / np.trapezoid(y, x) * 100.0   # normalized “percent-like” density over [0,1]\n",
        "\n",
        "    plt.fill_between(x, y, alpha=0.25)\n",
        "    plt.plot(x, y, linewidth=2,\n",
        "             label=f\"{tag} | ID {sid} | Score {mu:.2f} | CS {cs:.0f}\")\n",
        "\n",
        "plt.axvline(THRESH, linestyle=\"--\", linewidth=2, label=f\"Threshold {THRESH:.2f}\")\n",
        "\n",
        "plt.xlabel(\"Model MCI-to-AD score (mean of MCD)\")\n",
        "plt.ylabel(\"Score density (normalized, %)\")\n",
        "plt.xlim(0, 1)\n",
        "plt.ylim(0,2000)\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"upper right\", fontsize=9)\n",
        "plt.show()\n",
        "\n",
        "print(\"Selected A,B,C,D IDs:\", [A[\"subject_id\"], B[\"subject_id\"], C[\"subject_id\"], D[\"subject_id\"]])\n"
      ],
      "metadata": {
        "id": "RU_pX8FUt-by"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "E5DMIYWAytYV"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}